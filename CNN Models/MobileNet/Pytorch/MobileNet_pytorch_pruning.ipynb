{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "mikPhaepuwZ9",
        "IA4_lU1uqsVm",
        "WMOiOp0sqBVT",
        "PAUEwLCXup-b",
        "7eeLMiVXufEA",
        "8bSjIRpJgT40",
        "lXRVI_0JkuUK",
        "6EQ-T4S_vQSH",
        "hNTNFh-XvV-M",
        "zhlZcVV4vbIm",
        "fskQ11bpt2RR",
        "35-fHqu3quHn"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "mikPhaepuwZ9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pw-xSN-lfJgs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Subset, random_split\n",
        "from torchvision import models\n",
        "from torchvision.models import mobilenet_v2\n",
        "from tqdm import tqdm\n",
        "from torchsummary import summary\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kepDUFdlf_Eq",
        "outputId": "31de8259-7746-44a1-b5e8-b74872f4384b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the dataset"
      ],
      "metadata": {
        "id": "IA4_lU1uqsVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix the seed to ensure reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Data augmentation for training (applied only to the train dataset)\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)), # Resize to MobileNet input size\n",
        "    transforms.RandomHorizontalFlip(), # Flip horizontally\n",
        "    transforms.RandomRotation(15), # Random rotation 15 degress\n",
        "    transforms.ToTensor(), # Convert to tensor\n",
        "    transforms.Normalize((0.5,), (0.5,)) # Normalize (mean=0.5, std=0.5)\n",
        "])\n",
        "\n",
        "# No augmentation for validation/test (only resizing and normalization)\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "# Download CIFAR-10 dataset\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
        "test_data = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
        "\n",
        "# Split the test_data into test (6k) and dev (4k)\n",
        "test_size = 6000\n",
        "dev_size = 4000\n",
        "test_dataset, dev_dataset = random_split(test_data, [test_size, dev_size])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "# Print dataset sizes\n",
        "print(f\"Train set size: {len(train_dataset)}\")\n",
        "print(f\"Dev set size: {len(dev_dataset)}\")\n",
        "print(f\"Test set size: {len(test_dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgb6nShrqwXv",
        "outputId": "9a32e5a5-e050-4a0d-b980-05ca430da733"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:05<00:00, 30.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Train set size: 50000\n",
            "Dev set size: 4000\n",
            "Test set size: 6000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining the model architecture to load the pre-trained weights"
      ],
      "metadata": {
        "id": "WMOiOp0sqBVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained MobileNet model\n",
        "base_model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# Unfreeze some of the top layers\n",
        "for param in base_model.features[:-10].parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Modify the classifier for CIFAR-10 (10 classes)\n",
        "class MobileNetV2(nn.Module):\n",
        "    def __init__(self, base_model):\n",
        "        super(MobileNetV2, self).__init__()\n",
        "        self.features = base_model.features\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(1280, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(64, 10)  # CIFAR-10 has 10 classes\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "model = MobileNetV2(base_model)"
      ],
      "metadata": {
        "id": "822T_YTqqA5l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1768a01-8360-437e-fb00-2d34f747ed89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n",
            "100%|██████████| 13.6M/13.6M [00:00<00:00, 118MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_weights_path = '/content/drive/My Drive/saved_models/pytorch_models/mobilenet_cifar10.pth'\n",
        "# load the full model\n",
        "model.load_state_dict(torch.load(model_weights_path))\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "print(\"Model weights loaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8xRGDzjgJpY",
        "outputId": "7a0cce1e-2781-4fa2-a931-f2ab6f2f2c42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-ae4576b2bf67>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_weights_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model weights loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting device and model summary"
      ],
      "metadata": {
        "id": "PAUEwLCXup-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the model is on the correct device (cuda or cpu)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMM2pN6xmnoA",
        "outputId": "5c2b534a-59ec-4e90-9ad7-9c3632ea289a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MobileNetV2(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2dNormActivation(\n",
              "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU6(inplace=True)\n",
              "    )\n",
              "    (1): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (2): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
              "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (3): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (4): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (5): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (6): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (7): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (8): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (9): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (10): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (11): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (12): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (13): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (14): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (15): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (16): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (17): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (18): Conv2dNormActivation(\n",
              "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU6(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=1280, out_features=256, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
              "    (5): ReLU()\n",
              "    (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (7): Dropout(p=0.5, inplace=False)\n",
              "    (8): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (9): ReLU()\n",
              "    (10): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (11): Dropout(p=0.5, inplace=False)\n",
              "    (12): Linear(in_features=64, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model, (3, 224, 224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r34Rf6sdsEAN",
        "outputId": "8ef00328-2605-409b-825e-051a31e2a19d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 32, 112, 112]             864\n",
            "       BatchNorm2d-2         [-1, 32, 112, 112]              64\n",
            "             ReLU6-3         [-1, 32, 112, 112]               0\n",
            "            Conv2d-4         [-1, 32, 112, 112]             288\n",
            "       BatchNorm2d-5         [-1, 32, 112, 112]              64\n",
            "             ReLU6-6         [-1, 32, 112, 112]               0\n",
            "            Conv2d-7         [-1, 16, 112, 112]             512\n",
            "       BatchNorm2d-8         [-1, 16, 112, 112]              32\n",
            "  InvertedResidual-9         [-1, 16, 112, 112]               0\n",
            "           Conv2d-10         [-1, 96, 112, 112]           1,536\n",
            "      BatchNorm2d-11         [-1, 96, 112, 112]             192\n",
            "            ReLU6-12         [-1, 96, 112, 112]               0\n",
            "           Conv2d-13           [-1, 96, 56, 56]             864\n",
            "      BatchNorm2d-14           [-1, 96, 56, 56]             192\n",
            "            ReLU6-15           [-1, 96, 56, 56]               0\n",
            "           Conv2d-16           [-1, 24, 56, 56]           2,304\n",
            "      BatchNorm2d-17           [-1, 24, 56, 56]              48\n",
            " InvertedResidual-18           [-1, 24, 56, 56]               0\n",
            "           Conv2d-19          [-1, 144, 56, 56]           3,456\n",
            "      BatchNorm2d-20          [-1, 144, 56, 56]             288\n",
            "            ReLU6-21          [-1, 144, 56, 56]               0\n",
            "           Conv2d-22          [-1, 144, 56, 56]           1,296\n",
            "      BatchNorm2d-23          [-1, 144, 56, 56]             288\n",
            "            ReLU6-24          [-1, 144, 56, 56]               0\n",
            "           Conv2d-25           [-1, 24, 56, 56]           3,456\n",
            "      BatchNorm2d-26           [-1, 24, 56, 56]              48\n",
            " InvertedResidual-27           [-1, 24, 56, 56]               0\n",
            "           Conv2d-28          [-1, 144, 56, 56]           3,456\n",
            "      BatchNorm2d-29          [-1, 144, 56, 56]             288\n",
            "            ReLU6-30          [-1, 144, 56, 56]               0\n",
            "           Conv2d-31          [-1, 144, 28, 28]           1,296\n",
            "      BatchNorm2d-32          [-1, 144, 28, 28]             288\n",
            "            ReLU6-33          [-1, 144, 28, 28]               0\n",
            "           Conv2d-34           [-1, 32, 28, 28]           4,608\n",
            "      BatchNorm2d-35           [-1, 32, 28, 28]              64\n",
            " InvertedResidual-36           [-1, 32, 28, 28]               0\n",
            "           Conv2d-37          [-1, 192, 28, 28]           6,144\n",
            "      BatchNorm2d-38          [-1, 192, 28, 28]             384\n",
            "            ReLU6-39          [-1, 192, 28, 28]               0\n",
            "           Conv2d-40          [-1, 192, 28, 28]           1,728\n",
            "      BatchNorm2d-41          [-1, 192, 28, 28]             384\n",
            "            ReLU6-42          [-1, 192, 28, 28]               0\n",
            "           Conv2d-43           [-1, 32, 28, 28]           6,144\n",
            "      BatchNorm2d-44           [-1, 32, 28, 28]              64\n",
            " InvertedResidual-45           [-1, 32, 28, 28]               0\n",
            "           Conv2d-46          [-1, 192, 28, 28]           6,144\n",
            "      BatchNorm2d-47          [-1, 192, 28, 28]             384\n",
            "            ReLU6-48          [-1, 192, 28, 28]               0\n",
            "           Conv2d-49          [-1, 192, 28, 28]           1,728\n",
            "      BatchNorm2d-50          [-1, 192, 28, 28]             384\n",
            "            ReLU6-51          [-1, 192, 28, 28]               0\n",
            "           Conv2d-52           [-1, 32, 28, 28]           6,144\n",
            "      BatchNorm2d-53           [-1, 32, 28, 28]              64\n",
            " InvertedResidual-54           [-1, 32, 28, 28]               0\n",
            "           Conv2d-55          [-1, 192, 28, 28]           6,144\n",
            "      BatchNorm2d-56          [-1, 192, 28, 28]             384\n",
            "            ReLU6-57          [-1, 192, 28, 28]               0\n",
            "           Conv2d-58          [-1, 192, 14, 14]           1,728\n",
            "      BatchNorm2d-59          [-1, 192, 14, 14]             384\n",
            "            ReLU6-60          [-1, 192, 14, 14]               0\n",
            "           Conv2d-61           [-1, 64, 14, 14]          12,288\n",
            "      BatchNorm2d-62           [-1, 64, 14, 14]             128\n",
            " InvertedResidual-63           [-1, 64, 14, 14]               0\n",
            "           Conv2d-64          [-1, 384, 14, 14]          24,576\n",
            "      BatchNorm2d-65          [-1, 384, 14, 14]             768\n",
            "            ReLU6-66          [-1, 384, 14, 14]               0\n",
            "           Conv2d-67          [-1, 384, 14, 14]           3,456\n",
            "      BatchNorm2d-68          [-1, 384, 14, 14]             768\n",
            "            ReLU6-69          [-1, 384, 14, 14]               0\n",
            "           Conv2d-70           [-1, 64, 14, 14]          24,576\n",
            "      BatchNorm2d-71           [-1, 64, 14, 14]             128\n",
            " InvertedResidual-72           [-1, 64, 14, 14]               0\n",
            "           Conv2d-73          [-1, 384, 14, 14]          24,576\n",
            "      BatchNorm2d-74          [-1, 384, 14, 14]             768\n",
            "            ReLU6-75          [-1, 384, 14, 14]               0\n",
            "           Conv2d-76          [-1, 384, 14, 14]           3,456\n",
            "      BatchNorm2d-77          [-1, 384, 14, 14]             768\n",
            "            ReLU6-78          [-1, 384, 14, 14]               0\n",
            "           Conv2d-79           [-1, 64, 14, 14]          24,576\n",
            "      BatchNorm2d-80           [-1, 64, 14, 14]             128\n",
            " InvertedResidual-81           [-1, 64, 14, 14]               0\n",
            "           Conv2d-82          [-1, 384, 14, 14]          24,576\n",
            "      BatchNorm2d-83          [-1, 384, 14, 14]             768\n",
            "            ReLU6-84          [-1, 384, 14, 14]               0\n",
            "           Conv2d-85          [-1, 384, 14, 14]           3,456\n",
            "      BatchNorm2d-86          [-1, 384, 14, 14]             768\n",
            "            ReLU6-87          [-1, 384, 14, 14]               0\n",
            "           Conv2d-88           [-1, 64, 14, 14]          24,576\n",
            "      BatchNorm2d-89           [-1, 64, 14, 14]             128\n",
            " InvertedResidual-90           [-1, 64, 14, 14]               0\n",
            "           Conv2d-91          [-1, 384, 14, 14]          24,576\n",
            "      BatchNorm2d-92          [-1, 384, 14, 14]             768\n",
            "            ReLU6-93          [-1, 384, 14, 14]               0\n",
            "           Conv2d-94          [-1, 384, 14, 14]           3,456\n",
            "      BatchNorm2d-95          [-1, 384, 14, 14]             768\n",
            "            ReLU6-96          [-1, 384, 14, 14]               0\n",
            "           Conv2d-97           [-1, 96, 14, 14]          36,864\n",
            "      BatchNorm2d-98           [-1, 96, 14, 14]             192\n",
            " InvertedResidual-99           [-1, 96, 14, 14]               0\n",
            "          Conv2d-100          [-1, 576, 14, 14]          55,296\n",
            "     BatchNorm2d-101          [-1, 576, 14, 14]           1,152\n",
            "           ReLU6-102          [-1, 576, 14, 14]               0\n",
            "          Conv2d-103          [-1, 576, 14, 14]           5,184\n",
            "     BatchNorm2d-104          [-1, 576, 14, 14]           1,152\n",
            "           ReLU6-105          [-1, 576, 14, 14]               0\n",
            "          Conv2d-106           [-1, 96, 14, 14]          55,296\n",
            "     BatchNorm2d-107           [-1, 96, 14, 14]             192\n",
            "InvertedResidual-108           [-1, 96, 14, 14]               0\n",
            "          Conv2d-109          [-1, 576, 14, 14]          55,296\n",
            "     BatchNorm2d-110          [-1, 576, 14, 14]           1,152\n",
            "           ReLU6-111          [-1, 576, 14, 14]               0\n",
            "          Conv2d-112          [-1, 576, 14, 14]           5,184\n",
            "     BatchNorm2d-113          [-1, 576, 14, 14]           1,152\n",
            "           ReLU6-114          [-1, 576, 14, 14]               0\n",
            "          Conv2d-115           [-1, 96, 14, 14]          55,296\n",
            "     BatchNorm2d-116           [-1, 96, 14, 14]             192\n",
            "InvertedResidual-117           [-1, 96, 14, 14]               0\n",
            "          Conv2d-118          [-1, 576, 14, 14]          55,296\n",
            "     BatchNorm2d-119          [-1, 576, 14, 14]           1,152\n",
            "           ReLU6-120          [-1, 576, 14, 14]               0\n",
            "          Conv2d-121            [-1, 576, 7, 7]           5,184\n",
            "     BatchNorm2d-122            [-1, 576, 7, 7]           1,152\n",
            "           ReLU6-123            [-1, 576, 7, 7]               0\n",
            "          Conv2d-124            [-1, 160, 7, 7]          92,160\n",
            "     BatchNorm2d-125            [-1, 160, 7, 7]             320\n",
            "InvertedResidual-126            [-1, 160, 7, 7]               0\n",
            "          Conv2d-127            [-1, 960, 7, 7]         153,600\n",
            "     BatchNorm2d-128            [-1, 960, 7, 7]           1,920\n",
            "           ReLU6-129            [-1, 960, 7, 7]               0\n",
            "          Conv2d-130            [-1, 960, 7, 7]           8,640\n",
            "     BatchNorm2d-131            [-1, 960, 7, 7]           1,920\n",
            "           ReLU6-132            [-1, 960, 7, 7]               0\n",
            "          Conv2d-133            [-1, 160, 7, 7]         153,600\n",
            "     BatchNorm2d-134            [-1, 160, 7, 7]             320\n",
            "InvertedResidual-135            [-1, 160, 7, 7]               0\n",
            "          Conv2d-136            [-1, 960, 7, 7]         153,600\n",
            "     BatchNorm2d-137            [-1, 960, 7, 7]           1,920\n",
            "           ReLU6-138            [-1, 960, 7, 7]               0\n",
            "          Conv2d-139            [-1, 960, 7, 7]           8,640\n",
            "     BatchNorm2d-140            [-1, 960, 7, 7]           1,920\n",
            "           ReLU6-141            [-1, 960, 7, 7]               0\n",
            "          Conv2d-142            [-1, 160, 7, 7]         153,600\n",
            "     BatchNorm2d-143            [-1, 160, 7, 7]             320\n",
            "InvertedResidual-144            [-1, 160, 7, 7]               0\n",
            "          Conv2d-145            [-1, 960, 7, 7]         153,600\n",
            "     BatchNorm2d-146            [-1, 960, 7, 7]           1,920\n",
            "           ReLU6-147            [-1, 960, 7, 7]               0\n",
            "          Conv2d-148            [-1, 960, 7, 7]           8,640\n",
            "     BatchNorm2d-149            [-1, 960, 7, 7]           1,920\n",
            "           ReLU6-150            [-1, 960, 7, 7]               0\n",
            "          Conv2d-151            [-1, 320, 7, 7]         307,200\n",
            "     BatchNorm2d-152            [-1, 320, 7, 7]             640\n",
            "InvertedResidual-153            [-1, 320, 7, 7]               0\n",
            "          Conv2d-154           [-1, 1280, 7, 7]         409,600\n",
            "     BatchNorm2d-155           [-1, 1280, 7, 7]           2,560\n",
            "           ReLU6-156           [-1, 1280, 7, 7]               0\n",
            "AdaptiveAvgPool2d-157           [-1, 1280, 1, 1]               0\n",
            "          Linear-158                  [-1, 256]         327,936\n",
            "            ReLU-159                  [-1, 256]               0\n",
            "     BatchNorm1d-160                  [-1, 256]             512\n",
            "         Dropout-161                  [-1, 256]               0\n",
            "          Linear-162                  [-1, 128]          32,896\n",
            "            ReLU-163                  [-1, 128]               0\n",
            "     BatchNorm1d-164                  [-1, 128]             256\n",
            "         Dropout-165                  [-1, 128]               0\n",
            "          Linear-166                   [-1, 64]           8,256\n",
            "            ReLU-167                   [-1, 64]               0\n",
            "     BatchNorm1d-168                   [-1, 64]             128\n",
            "         Dropout-169                   [-1, 64]               0\n",
            "          Linear-170                   [-1, 10]             650\n",
            "================================================================\n",
            "Total params: 2,594,506\n",
            "Trainable params: 2,463,690\n",
            "Non-trainable params: 130,816\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 152.87\n",
            "Params size (MB): 9.90\n",
            "Estimated Total Size (MB): 163.34\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Required functions and classes (Evaluate and train and mask enforcing class)"
      ],
      "metadata": {
        "id": "7eeLMiVXufEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader, criterion, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    test_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():  # No gradients for validation (Disable gradient calculations for efficiency)\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Update loss\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            # Get predictions\n",
        "            _, predicted = outputs.max(1)\n",
        "\n",
        "            # Update metrics\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "    end_time = time.time()\n",
        "    inference_time = end_time - start_time\n",
        "    avg_loss = test_loss / len(test_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "\n",
        "    #print(f\"Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.2f}%\")\n",
        "    return avg_loss, accuracy, inference_time"
      ],
      "metadata": {
        "id": "FIUh6id9i782"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Class to enforce pruning masks during fine-tuning\n",
        "class MaskEnforcer:\n",
        "    def __init__(self, model, masks):\n",
        "        self.model = model\n",
        "        self.masks = masks\n",
        "\n",
        "    def enforce(self):\n",
        "        with torch.no_grad():\n",
        "            for name, module in self.model.named_modules():\n",
        "                if name in self.masks and isinstance(module, (nn.Conv2d, nn.Linear)):\n",
        "                    module.weight *= self.masks[name]"
      ],
      "metadata": {
        "id": "X1pKFMJg_ACy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "def train_model(model, train_loader, dev_loader, criterion, optimizer, num_epochs=10, mask_enforcer=None):\n",
        "    for epoch in range(num_epochs):\n",
        "        ### Training Phase ###\n",
        "        model.train()  # Set model to training mode\n",
        "        running_loss = 0.0\n",
        "        correct, total = 0, 0\n",
        "\n",
        "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=True)\n",
        "        for images, labels in loop:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Enforce pruning mask if provided\n",
        "            if mask_enforcer:\n",
        "                mask_enforcer.enforce()\n",
        "\n",
        "            # Compute training metrics\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            # Update progress bar\n",
        "            loop.set_postfix(train_loss=running_loss / total, train_acc=100. * correct / total)\n",
        "\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_acc = 100. * correct / total\n",
        "\n",
        "        ### Validation (Dev) Phase ###\n",
        "        model.eval()  # Set model to evaluation mode\n",
        "        dev_loss, dev_correct, dev_total = 0.0, 0, 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in dev_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                dev_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                dev_total += labels.size(0)\n",
        "                dev_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        dev_loss /= len(dev_loader)\n",
        "        dev_acc = 100. * dev_correct / dev_total\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Dev Loss: {dev_loss:.4f}, Dev Acc: {dev_acc:.2f}%\")\n",
        "\n",
        "    print(\"Training complete!\")"
      ],
      "metadata": {
        "id": "r3Ek2uxBukRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation (before pruning)"
      ],
      "metadata": {
        "id": "8bSjIRpJgT40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "avgLoss, acc, inf_time = evaluate_model(model, test_loader, criterion, device)\n",
        "# Print evaluation results\n",
        "print(f\"Average test Loss: {avgLoss:.4f}, Test Accuracy: {acc:.2f}%, Inference Time for {len(test_dataset)} images: {inf_time:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZxRHKkXgRRO",
        "outputId": "eabefb06-0aa4-4d26-c715-00420c311c4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average test Loss: 0.2781, Test Accuracy: 91.35%, Inference Time for 6000 images: 13.13 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pruning"
      ],
      "metadata": {
        "id": "lXRVI_0JkuUK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## See weights before pruning"
      ],
      "metadata": {
        "id": "6EQ-T4S_vQSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the first convolutional layer\n",
        "conv1 = model.features[0]\n",
        "print(conv1)\n",
        "\n",
        "# Inspect its parameters (weights and bias)\n",
        "print(list(conv1.named_parameters()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3j5klMQLssY-",
        "outputId": "7a631c69-ba63-4ecb-ca5c-4547ee8c932a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conv2dNormActivation(\n",
            "  (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (2): ReLU6(inplace=True)\n",
            ")\n",
            "[('0.weight', Parameter containing:\n",
            "tensor([[[[ 1.3185e-02, -4.3213e-03,  1.4823e-02],\n",
            "          [ 3.2780e-02, -2.5385e-02,  6.8572e-03],\n",
            "          [ 1.0549e-02, -3.7347e-02, -1.4727e-02]],\n",
            "\n",
            "         [[ 7.9917e-03, -5.9146e-03,  1.5076e-02],\n",
            "          [ 1.9999e-02, -3.2863e-02, -2.0859e-03],\n",
            "          [ 1.1350e-02, -3.2956e-02, -7.8733e-03]],\n",
            "\n",
            "         [[-2.5234e-02, -2.0167e-02, -9.9620e-03],\n",
            "          [-1.1213e-02, -2.9266e-02, -1.5218e-02],\n",
            "          [-2.6531e-02, -3.3449e-02, -2.4215e-02]]],\n",
            "\n",
            "\n",
            "        [[[-6.3694e-02, -2.1300e-02,  2.1416e-02],\n",
            "          [ 1.3037e-01,  3.7967e-01,  4.1983e-02],\n",
            "          [-1.8793e-01, -2.7921e-01, -3.8335e-02]],\n",
            "\n",
            "         [[-6.7306e-02,  5.3145e-02,  1.8353e-03],\n",
            "          [ 2.6662e-01,  7.4863e-01,  1.0166e-01],\n",
            "          [-3.3641e-01, -6.0844e-01, -7.6828e-02]],\n",
            "\n",
            "         [[-1.5798e-02, -1.0921e-02,  2.6319e-02],\n",
            "          [ 4.2062e-02,  1.7312e-01,  1.5455e-03],\n",
            "          [-6.2128e-02, -1.5648e-01,  1.0293e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.0511e-01,  9.7661e-02, -7.8915e-03],\n",
            "          [-4.0477e-01,  7.3995e-01, -3.1516e-01],\n",
            "          [-6.7326e-01,  9.2475e-01, -2.9982e-01]],\n",
            "\n",
            "         [[-1.2098e-01,  1.9127e-01, -4.2251e-02],\n",
            "          [-8.2502e-01,  1.3440e+00, -5.3651e-01],\n",
            "          [-1.0619e+00,  1.4805e+00, -4.7216e-01]],\n",
            "\n",
            "         [[-2.1906e-02, -2.1872e-02,  3.5652e-02],\n",
            "          [-2.0223e-01,  3.7674e-01, -1.7207e-01],\n",
            "          [-2.7817e-01,  4.3454e-01, -1.5861e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 2.3033e-03, -4.4324e-02,  8.3432e-02],\n",
            "          [ 6.0629e-02,  1.9979e-01, -2.8505e-01],\n",
            "          [ 1.9608e-02,  3.0920e-01, -3.3644e-01]],\n",
            "\n",
            "         [[-9.6925e-03, -6.8256e-02,  8.4860e-02],\n",
            "          [ 4.8464e-02,  5.2001e-01, -6.1890e-01],\n",
            "          [ 5.1091e-02,  6.4870e-01, -7.0560e-01]],\n",
            "\n",
            "         [[-1.7378e-02, -2.9058e-02,  2.5477e-02],\n",
            "          [ 4.3098e-02,  8.5977e-02, -6.9092e-02],\n",
            "          [-1.0171e-03,  9.0270e-02, -1.2385e-01]]],\n",
            "\n",
            "\n",
            "        [[[-6.9104e-02,  2.6336e-02,  2.7510e-02],\n",
            "          [ 9.4920e-03, -8.0321e-02, -7.5000e-02],\n",
            "          [ 1.1584e-01, -1.0810e-01, -3.0890e-01]],\n",
            "\n",
            "         [[ 2.0667e-02,  3.7113e-02, -1.3271e-02],\n",
            "          [ 7.5568e-02, -2.0804e-01, -2.5877e-01],\n",
            "          [ 2.0265e-01, -2.1382e-01, -5.1192e-01]],\n",
            "\n",
            "         [[-7.4568e-04,  2.5926e-02, -3.5042e-02],\n",
            "          [-6.4727e-03, -5.9524e-02, -6.1520e-02],\n",
            "          [ 2.5698e-02, -5.6478e-02, -1.0738e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 1.5896e-01,  1.6877e-01, -2.4298e-01],\n",
            "          [ 1.2398e-01,  6.4718e-01,  2.4875e-01],\n",
            "          [-1.5780e-01,  6.2026e-01,  6.6875e-01]],\n",
            "\n",
            "         [[ 8.7203e-03, -1.3609e-01,  5.0322e-03],\n",
            "          [-3.7221e-02, -2.5028e-01, -1.4688e-01],\n",
            "          [-2.2397e-02, -2.0097e-01, -1.4839e-01]],\n",
            "\n",
            "         [[-1.4165e-01, -6.0620e-02,  2.3864e-01],\n",
            "          [-3.8656e-02, -3.9749e-01, -1.1775e-01],\n",
            "          [ 1.6875e-01, -3.8926e-01, -4.7925e-01]]],\n",
            "\n",
            "\n",
            "        [[[-1.9617e-02, -2.0635e-02, -1.3567e-03],\n",
            "          [ 4.5140e-01, -1.4004e-01, -2.1267e-01],\n",
            "          [ 1.2382e-01, -1.2372e-01, -7.9849e-02]],\n",
            "\n",
            "         [[-6.2512e-02, -1.5294e-02,  2.0405e-02],\n",
            "          [ 9.2213e-01, -2.7301e-01, -4.6281e-01],\n",
            "          [ 2.4823e-01, -2.4583e-01, -9.6681e-02]],\n",
            "\n",
            "         [[ 1.1864e-02,  1.1375e-02, -2.6585e-03],\n",
            "          [ 2.2752e-01, -6.9514e-02, -1.1689e-01],\n",
            "          [ 5.5251e-02, -4.8208e-02, -2.4621e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 2.3331e-02, -2.1255e-02, -1.2043e-01],\n",
            "          [-1.3311e-02,  8.5699e-02,  2.2260e-01],\n",
            "          [-1.3874e-02,  1.5130e-01,  3.6512e-01]],\n",
            "\n",
            "         [[-1.9823e-02, -9.4805e-02, -1.8251e-01],\n",
            "          [ 7.8131e-03,  1.9028e-01,  3.6958e-01],\n",
            "          [ 6.4962e-03,  3.4880e-01,  5.8963e-01]],\n",
            "\n",
            "         [[-8.2381e-03, -2.2187e-02, -4.0889e-02],\n",
            "          [ 4.6145e-03,  1.7125e-02,  1.0749e-01],\n",
            "          [ 2.0155e-02,  7.3647e-02,  1.3677e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 5.2242e-02,  4.3457e-02, -1.3176e-01],\n",
            "          [-1.8243e-01,  6.9045e-01, -5.1952e-01],\n",
            "          [ 2.3658e-02,  7.2798e-01, -6.8826e-01]],\n",
            "\n",
            "         [[-3.7846e-02,  1.6695e-01, -2.0825e-01],\n",
            "          [-1.2891e-01,  1.0871e+00, -9.2057e-01],\n",
            "          [ 1.2196e-02,  1.0470e+00, -9.8275e-01]],\n",
            "\n",
            "         [[ 4.8951e-02, -1.4226e-02, -6.8652e-03],\n",
            "          [-1.1681e-01,  3.9946e-01, -3.3150e-01],\n",
            "          [ 8.7250e-03,  3.9962e-01, -3.9028e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 7.2973e-05,  2.9785e-03,  3.3676e-03],\n",
            "          [ 1.0816e-03,  3.2872e-04,  1.9929e-03],\n",
            "          [ 4.8153e-04, -6.9890e-04,  1.0293e-03]],\n",
            "\n",
            "         [[ 1.6370e-03,  2.2205e-03,  1.2631e-03],\n",
            "          [ 1.6240e-03,  1.4388e-03,  2.4364e-03],\n",
            "          [ 1.2906e-03,  5.2904e-04,  2.8437e-03]],\n",
            "\n",
            "         [[ 5.7504e-03,  4.4944e-03,  4.4141e-03],\n",
            "          [ 3.2225e-03,  3.1395e-03,  5.3399e-03],\n",
            "          [ 2.0441e-03,  2.6774e-03,  2.0193e-03]]],\n",
            "\n",
            "\n",
            "        [[[-5.6621e-02, -1.7057e-01, -9.7005e-02],\n",
            "          [-2.3219e-01, -2.5373e-01, -8.8598e-02],\n",
            "          [ 2.6555e-02, -8.7591e-03, -6.6266e-02]],\n",
            "\n",
            "         [[ 2.4311e-01,  2.8632e-01,  2.8954e-01],\n",
            "          [ 5.5644e-02,  1.4815e-01,  2.6344e-01],\n",
            "          [ 2.5410e-01,  3.3360e-01,  1.8493e-01]],\n",
            "\n",
            "         [[ 2.6762e-02, -1.5172e-01, -2.4075e-01],\n",
            "          [-2.4401e-01, -3.3433e-01, -3.0042e-01],\n",
            "          [-4.6951e-02, -7.5674e-02, -1.7909e-01]]],\n",
            "\n",
            "\n",
            "        [[[-9.4053e-02,  2.4103e-02,  5.9160e-02],\n",
            "          [ 3.0418e-02,  9.5559e-02, -2.0265e-02],\n",
            "          [-9.0549e-02,  3.0509e-01,  2.3330e-01]],\n",
            "\n",
            "         [[ 4.7231e-03,  5.3942e-02, -1.7286e-02],\n",
            "          [ 8.1389e-03,  1.4094e-01, -5.9862e-02],\n",
            "          [-1.2946e-01,  4.3589e-01,  4.3022e-01]],\n",
            "\n",
            "         [[-1.0500e-02, -4.2589e-03, -1.4456e-03],\n",
            "          [ 2.3868e-02,  2.4256e-02, -3.8060e-02],\n",
            "          [-1.1281e-01,  1.7584e-01,  1.3310e-01]]],\n",
            "\n",
            "\n",
            "        [[[-1.5790e-01, -3.1480e-01, -8.1600e-02],\n",
            "          [ 2.4550e-01,  3.3041e-01,  8.6316e-02],\n",
            "          [-5.1641e-02, -4.4483e-02,  2.4532e-02]],\n",
            "\n",
            "         [[-3.1101e-01, -6.5119e-01, -2.1308e-01],\n",
            "          [ 5.0131e-01,  6.5705e-01,  1.6352e-01],\n",
            "          [-1.1949e-01, -1.0149e-01, -1.6246e-02]],\n",
            "\n",
            "         [[-5.7527e-02, -1.2403e-01, -3.1133e-02],\n",
            "          [ 1.0915e-01,  1.4117e-01,  2.4606e-02],\n",
            "          [-2.3635e-02, -2.8387e-02,  1.8276e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 4.7425e-01,  1.7911e-02,  9.0779e-03],\n",
            "          [ 7.8593e-02, -7.2171e-02,  4.7412e-04],\n",
            "          [ 1.9806e-02, -2.0749e-04, -2.3568e-02]],\n",
            "\n",
            "         [[ 5.7808e-01,  8.0209e-02,  4.2801e-02],\n",
            "          [ 1.0537e-02, -5.0667e-02,  5.7978e-02],\n",
            "          [-4.9334e-02,  2.1495e-03, -3.8234e-02]],\n",
            "\n",
            "         [[ 2.6621e-01, -8.1927e-02, -4.6313e-02],\n",
            "          [-1.4452e-01, -8.3210e-02, -2.5840e-03],\n",
            "          [ 1.9681e-02,  1.0334e-01, -3.7704e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 2.4720e-03,  9.0920e-04,  2.0388e-03],\n",
            "          [ 6.1540e-04,  3.8974e-03,  1.4280e-04],\n",
            "          [ 3.1730e-03,  2.0194e-03,  2.6482e-03]],\n",
            "\n",
            "         [[-1.0876e-04,  1.1112e-03, -4.2103e-04],\n",
            "          [ 8.4845e-04, -4.3134e-04, -2.7269e-04],\n",
            "          [ 2.4061e-03,  8.7139e-04, -2.7488e-04]],\n",
            "\n",
            "         [[ 1.3895e-03, -3.8223e-04, -2.7434e-04],\n",
            "          [ 1.1153e-03,  8.2224e-05, -5.4411e-04],\n",
            "          [ 1.2282e-03,  2.1229e-03, -1.9035e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 2.3364e-03,  3.6554e-03,  4.0789e-03],\n",
            "          [ 4.5015e-03,  3.0230e-03,  3.0200e-03],\n",
            "          [ 4.0525e-03,  7.1955e-04,  1.9904e-03]],\n",
            "\n",
            "         [[-1.5098e-03, -1.1016e-03, -1.7503e-03],\n",
            "          [-1.2978e-03,  3.3319e-04,  1.1286e-03],\n",
            "          [ 4.5001e-04,  6.2764e-04,  6.3853e-04]],\n",
            "\n",
            "         [[ 9.1040e-04, -1.0552e-03, -6.4881e-04],\n",
            "          [ 3.5650e-04,  4.7908e-04,  3.8994e-05],\n",
            "          [ 1.6581e-03,  1.6407e-03, -2.5550e-04]]],\n",
            "\n",
            "\n",
            "        [[[-2.4044e-04,  4.8170e-04,  4.2559e-03],\n",
            "          [ 1.2776e-03,  9.3743e-03,  2.0873e-02],\n",
            "          [ 5.9802e-04, -2.4796e-03,  1.3691e-03]],\n",
            "\n",
            "         [[-2.2936e-03, -1.8581e-03,  7.5235e-03],\n",
            "          [-1.4675e-03,  1.4854e-02,  2.9661e-02],\n",
            "          [-4.6998e-03, -2.3057e-03, -2.8649e-04]],\n",
            "\n",
            "         [[ 1.3620e-03, -4.9935e-03, -1.0982e-03],\n",
            "          [-1.0637e-03,  3.1088e-03,  1.8076e-02],\n",
            "          [-1.0242e-04, -7.1458e-03, -6.9054e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 7.8284e-04,  1.5379e-03,  1.4106e-03],\n",
            "          [-9.3499e-04, -2.7127e-03,  1.5881e-04],\n",
            "          [-1.5515e-03, -1.6176e-03,  2.1503e-04]],\n",
            "\n",
            "         [[-3.1531e-03, -1.7857e-03, -1.1797e-03],\n",
            "          [-3.2941e-03, -5.4476e-03, -1.7999e-04],\n",
            "          [-4.9098e-03, -3.7540e-03, -6.0913e-03]],\n",
            "\n",
            "         [[ 1.9296e-03,  5.3232e-04,  4.6620e-04],\n",
            "          [ 5.5724e-04, -2.1065e-03,  3.0727e-04],\n",
            "          [-1.6903e-03, -1.4307e-03, -6.7279e-04]]],\n",
            "\n",
            "\n",
            "        [[[ 5.8486e-02, -1.2825e-02, -3.8267e-02],\n",
            "          [-2.6494e-02,  4.5126e-03,  3.4520e-02],\n",
            "          [-5.8972e-02,  7.8444e-02,  1.8526e-01]],\n",
            "\n",
            "         [[ 2.8463e-02, -2.6931e-02, -1.0884e-01],\n",
            "          [-1.5787e-02,  1.1329e-01,  3.3758e-02],\n",
            "          [-6.4415e-02,  2.1384e-01,  2.5013e-01]],\n",
            "\n",
            "         [[ 1.5934e-02,  7.3900e-03, -4.3540e-02],\n",
            "          [-1.7625e-02, -2.4394e-02,  2.5213e-02],\n",
            "          [-1.0826e-02,  1.3342e-02,  1.4850e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 1.1236e-02, -2.0795e-01, -2.5906e-01],\n",
            "          [-2.1665e-01, -4.6541e-01, -5.2209e-01],\n",
            "          [-1.1556e-01, -2.8515e-01, -3.2759e-01]],\n",
            "\n",
            "         [[ 1.2261e-01,  1.1908e-01,  9.5306e-02],\n",
            "          [ 8.4748e-02,  7.3614e-02,  6.0823e-02],\n",
            "          [ 1.2698e-01,  8.7237e-02,  4.8038e-02]],\n",
            "\n",
            "         [[-1.1349e-01,  9.6276e-02,  1.1627e-01],\n",
            "          [ 8.5351e-02,  4.2268e-01,  4.5017e-01],\n",
            "          [ 1.6465e-02,  2.7059e-01,  2.4190e-01]]],\n",
            "\n",
            "\n",
            "        [[[-1.6468e-02, -3.2096e-01,  3.9619e-03],\n",
            "          [ 1.7744e-02, -1.6314e-01, -7.0905e-03],\n",
            "          [ 1.0061e-01,  9.2573e-02,  1.2323e-01]],\n",
            "\n",
            "         [[-2.1632e-01, -3.4755e-01, -1.3225e-01],\n",
            "          [-8.8979e-02, -9.3173e-02, -7.8098e-02],\n",
            "          [-1.1856e-01,  2.9274e-02, -5.2220e-02]],\n",
            "\n",
            "         [[ 1.9912e-01, -1.4410e-01,  1.3091e-01],\n",
            "          [ 5.9623e-02, -1.2904e-01, -1.8040e-03],\n",
            "          [ 1.0897e-02,  2.1879e-02,  3.9159e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.3218e-01, -8.5662e-02,  2.6492e-01],\n",
            "          [ 2.3961e-01, -2.9687e-01, -4.8428e-02],\n",
            "          [-1.1590e-01,  6.3784e-02, -8.6523e-02]],\n",
            "\n",
            "         [[-1.1151e-01, -1.5637e-01,  4.6423e-01],\n",
            "          [ 2.9102e-01, -5.6687e-01, -6.6126e-02],\n",
            "          [-1.3174e-01, -7.1403e-03, -1.4485e-01]],\n",
            "\n",
            "         [[-8.5986e-02,  2.7117e-03,  1.0288e-01],\n",
            "          [ 1.5779e-01, -1.6759e-01,  2.4043e-02],\n",
            "          [-1.0756e-01,  1.3325e-01, -9.9100e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 7.1742e-02,  2.0425e-01,  1.6603e-01],\n",
            "          [ 3.5042e-01,  3.7528e-01,  1.9535e-01],\n",
            "          [-1.4909e-01, -2.1903e-02,  2.9302e-01]],\n",
            "\n",
            "         [[-2.7327e-01, -4.4579e-01, -3.4393e-01],\n",
            "          [-1.2411e-01, -4.4586e-01, -5.6728e-01],\n",
            "          [-3.5071e-01, -5.4266e-01, -3.3191e-01]],\n",
            "\n",
            "         [[-1.0466e-01,  2.7065e-01,  3.2107e-01],\n",
            "          [ 4.4540e-01,  6.9639e-01,  4.4503e-01],\n",
            "          [ 1.1907e-01,  1.7708e-01,  1.8877e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 1.9949e-03,  7.0329e-04,  1.2564e-04],\n",
            "          [ 1.1142e-03,  3.7291e-04, -1.8765e-03],\n",
            "          [ 4.7057e-04, -3.0855e-03, -5.0659e-03]],\n",
            "\n",
            "         [[ 3.7316e-03,  2.0575e-03,  4.4560e-04],\n",
            "          [ 4.8448e-03,  2.7835e-03,  1.2320e-03],\n",
            "          [ 7.3089e-04,  5.7752e-04, -8.0281e-04]],\n",
            "\n",
            "         [[ 3.6424e-03,  4.0336e-03,  1.4712e-03],\n",
            "          [ 6.4370e-03,  2.3032e-03, -7.5608e-05],\n",
            "          [ 1.8066e-05, -1.2086e-04, -3.7913e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 5.9717e-02,  6.9803e-02,  1.3365e-02],\n",
            "          [-1.1119e-02, -8.3796e-02, -1.1948e-01],\n",
            "          [-3.4030e-02,  9.1925e-03,  1.0911e-01]],\n",
            "\n",
            "         [[ 1.2852e-01, -9.4624e-03, -1.9692e-01],\n",
            "          [ 5.9145e-02, -1.5371e-01, -2.6794e-01],\n",
            "          [ 5.9304e-02,  8.4681e-02,  1.9035e-01]],\n",
            "\n",
            "         [[-1.5515e-01, -1.5892e-01, -3.6346e-01],\n",
            "          [-2.2866e-02, -9.8529e-02, -2.4052e-01],\n",
            "          [-1.1325e-01,  1.2810e-02,  3.0225e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.4643e-02,  3.6421e-02, -2.1607e-02],\n",
            "          [-1.1684e-01,  3.1353e-01,  5.7411e-01],\n",
            "          [ 8.8140e-02, -3.2238e-01, -5.2594e-01]],\n",
            "\n",
            "         [[ 3.2538e-02,  6.2433e-02,  1.5983e-02],\n",
            "          [-1.9405e-01,  5.2397e-01,  9.7491e-01],\n",
            "          [ 1.5695e-01, -6.0305e-01, -9.5559e-01]],\n",
            "\n",
            "         [[ 4.1030e-02, -2.5530e-02, -1.9472e-02],\n",
            "          [-6.5120e-02,  1.5390e-01,  2.7912e-01],\n",
            "          [ 5.5378e-02, -1.8685e-01, -2.2544e-01]]],\n",
            "\n",
            "\n",
            "        [[[-7.1688e-03,  5.3040e-02, -5.8645e-02],\n",
            "          [ 4.2730e-02, -8.6069e-02, -9.6551e-02],\n",
            "          [ 8.4399e-03, -6.2775e-02, -2.4073e-01]],\n",
            "\n",
            "         [[ 5.8025e-02,  2.9430e-02, -1.6300e-02],\n",
            "          [ 6.5350e-02, -2.5471e-01, -2.0890e-01],\n",
            "          [ 1.3599e-01, -1.6682e-01, -3.4076e-01]],\n",
            "\n",
            "         [[-4.7203e-02,  6.7784e-02,  9.0382e-02],\n",
            "          [ 1.1720e-02, -3.0898e-03,  5.3827e-03],\n",
            "          [ 1.0204e-02,  1.7147e-02, -1.5998e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 2.0258e-01,  6.3044e-02, -3.3919e-02],\n",
            "          [-2.2679e-01, -1.2839e-01,  1.4155e-01],\n",
            "          [ 2.1057e-01,  2.1404e-01,  2.3188e-02]],\n",
            "\n",
            "         [[ 2.2042e-01, -1.8370e-01, -1.7563e-01],\n",
            "          [-6.8805e-01, -7.6530e-01, -2.7512e-01],\n",
            "          [ 2.4695e-01,  1.5930e-01, -1.2255e-01]],\n",
            "\n",
            "         [[ 1.1005e-01,  7.3410e-02,  1.0418e-02],\n",
            "          [-1.5275e-01,  1.2108e-02,  1.1410e-01],\n",
            "          [ 1.4600e-01,  2.1541e-01, -4.6493e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 2.8727e-02,  7.0820e-02,  1.7192e-01],\n",
            "          [ 8.5161e-02,  8.0525e-02,  1.4091e-01],\n",
            "          [ 3.5461e-02, -1.5672e-01, -6.7180e-02]],\n",
            "\n",
            "         [[-2.9276e-02, -5.5944e-02, -2.5166e-02],\n",
            "          [ 4.4830e-02, -7.6599e-02, -1.1612e-02],\n",
            "          [ 9.9829e-02, -1.3302e-01,  6.3633e-02]],\n",
            "\n",
            "         [[-1.0342e-01, -8.3423e-02, -1.1177e-01],\n",
            "          [-1.1350e-01, -2.5592e-01, -2.3260e-01],\n",
            "          [-3.9492e-02, -2.7209e-01, -1.1122e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 1.2080e-02, -5.6736e-03,  1.2121e-02],\n",
            "          [ 3.6384e-02, -4.7567e-02, -8.5535e-02],\n",
            "          [ 1.3808e-01, -3.2317e-02, -1.4576e-01]],\n",
            "\n",
            "         [[ 4.4679e-02,  1.6687e-02,  4.8820e-02],\n",
            "          [ 3.2228e-02, -1.1162e-01, -1.5312e-01],\n",
            "          [ 1.5950e-01, -6.7039e-02, -1.9372e-01]],\n",
            "\n",
            "         [[ 7.5279e-03,  2.2123e-02,  7.6448e-02],\n",
            "          [ 5.8242e-02, -2.4968e-02, -3.8511e-02],\n",
            "          [ 1.5056e-01, -5.8731e-02, -1.7098e-01]]],\n",
            "\n",
            "\n",
            "        [[[-2.7416e-01,  4.3190e-02,  3.1955e-01],\n",
            "          [ 2.6226e-01,  7.3638e-01,  7.7162e-01],\n",
            "          [ 3.9359e-01,  5.2800e-01,  1.7348e-01]],\n",
            "\n",
            "         [[-2.5607e-02, -1.4500e-01, -1.5820e-01],\n",
            "          [-1.7200e-01, -3.3076e-01, -3.9672e-01],\n",
            "          [-2.0911e-01, -3.4196e-01, -3.4588e-01]],\n",
            "\n",
            "         [[ 2.5432e-01,  9.2269e-02, -1.5773e-01],\n",
            "          [ 1.7725e-03, -2.0813e-01, -2.7601e-01],\n",
            "          [-2.6269e-01, -2.1233e-01,  1.2211e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 1.8198e-03,  1.5130e-02, -2.8492e-02],\n",
            "          [-7.8950e-02,  2.0811e-03,  1.2064e-01],\n",
            "          [-3.1852e-02,  2.7805e-02,  1.8340e-01]],\n",
            "\n",
            "         [[ 2.3251e-02,  1.5515e-02, -2.2675e-02],\n",
            "          [-1.5360e-02,  5.0303e-02,  1.7445e-01],\n",
            "          [ 3.7096e-02,  7.4813e-02,  2.4061e-01]],\n",
            "\n",
            "         [[-2.2877e-02, -3.7378e-02, -1.2813e-01],\n",
            "          [-5.0640e-02,  2.3793e-02,  6.0575e-02],\n",
            "          [-6.8655e-03,  4.2237e-02,  1.0798e-01]]]], device='cuda:0')), ('1.weight', Parameter containing:\n",
            "tensor([0.0381, 0.1872, 0.1975, 0.2451, 0.1313, 0.1590, 0.0881, 0.2552, 0.0870,\n",
            "        0.0119, 0.4129, 0.1137, 0.2245, 0.3014, 0.0114, 0.0104, 0.0128, 0.0043,\n",
            "        0.0668, 0.4108, 0.3578, 0.1278, 0.5250, 0.0039, 0.4444, 0.1298, 0.3284,\n",
            "        0.2453, 0.3565, 0.3066, 0.4146, 0.1419], device='cuda:0')), ('1.bias', Parameter containing:\n",
            "tensor([-8.4354e-02,  5.6023e-01,  3.5002e-01,  2.8363e-01,  9.7327e-01,\n",
            "         6.4774e-01,  4.9481e-01,  5.5817e-01,  6.1756e-01, -4.2980e-04,\n",
            "        -3.0858e-01,  9.5334e-01,  4.4609e-01, -3.8414e-01, -9.3045e-04,\n",
            "         5.7470e-03, -4.2064e-02, -1.7965e-02,  3.3821e-01,  1.1017e-01,\n",
            "        -2.5284e-01,  5.0251e-01,  3.7990e-01, -1.5532e-02, -4.6869e-01,\n",
            "         5.1056e-01, -2.8880e-01,  6.4006e-01, -1.0935e-01, -5.9483e-02,\n",
            "         3.7479e-01,  2.6511e-01], device='cuda:0'))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply Local unstructured pruning (magnitude pruning) (L1)"
      ],
      "metadata": {
        "id": "hNTNFh-XvV-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.utils.prune as prune\n",
        "\n",
        "# Store pruning masks\n",
        "pruned_masks = {}\n",
        "\n",
        "# Apply and store pruning masks\n",
        "def prune_model(model, amount=0.2):\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
        "            prune.l1_unstructured(module, name=\"weight\", amount=amount)\n",
        "            # Save the mask for future enforcement\n",
        "            pruned_masks[name] = (module.weight != 0).float()\n",
        "            # Make pruning permanent\n",
        "            prune.remove(module, \"weight\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "hDbvgOZWs6E6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = prune_model(model, amount=0.5)  # Prune 30% of Conv2d and Linear layers"
      ],
      "metadata": {
        "id": "SLWS4Qu0s9pV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pruned_masks.keys())  # Check which layers were pruned\n",
        "print(pruned_masks['features.0.0'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFHheW6fHnmF",
        "outputId": "81c7d0dc-7274-466a-ab6a-84d266997d15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['features.0.0', 'features.1.conv.0.0', 'features.1.conv.1', 'features.2.conv.0.0', 'features.2.conv.1.0', 'features.2.conv.2', 'features.3.conv.0.0', 'features.3.conv.1.0', 'features.3.conv.2', 'features.4.conv.0.0', 'features.4.conv.1.0', 'features.4.conv.2', 'features.5.conv.0.0', 'features.5.conv.1.0', 'features.5.conv.2', 'features.6.conv.0.0', 'features.6.conv.1.0', 'features.6.conv.2', 'features.7.conv.0.0', 'features.7.conv.1.0', 'features.7.conv.2', 'features.8.conv.0.0', 'features.8.conv.1.0', 'features.8.conv.2', 'features.9.conv.0.0', 'features.9.conv.1.0', 'features.9.conv.2', 'features.10.conv.0.0', 'features.10.conv.1.0', 'features.10.conv.2', 'features.11.conv.0.0', 'features.11.conv.1.0', 'features.11.conv.2', 'features.12.conv.0.0', 'features.12.conv.1.0', 'features.12.conv.2', 'features.13.conv.0.0', 'features.13.conv.1.0', 'features.13.conv.2', 'features.14.conv.0.0', 'features.14.conv.1.0', 'features.14.conv.2', 'features.15.conv.0.0', 'features.15.conv.1.0', 'features.15.conv.2', 'features.16.conv.0.0', 'features.16.conv.1.0', 'features.16.conv.2', 'features.17.conv.0.0', 'features.17.conv.1.0', 'features.17.conv.2', 'features.18.0', 'classifier.0', 'classifier.4', 'classifier.8', 'classifier.12'])\n",
            "tensor([[[[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[1., 0., 0.],\n",
            "          [1., 1., 0.],\n",
            "          [1., 1., 0.]],\n",
            "\n",
            "         [[1., 0., 0.],\n",
            "          [1., 1., 1.],\n",
            "          [1., 1., 1.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 1., 0.],\n",
            "          [1., 1., 0.]]],\n",
            "\n",
            "\n",
            "        [[[1., 1., 0.],\n",
            "          [1., 1., 1.],\n",
            "          [1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 0.],\n",
            "          [1., 1., 1.],\n",
            "          [1., 1., 1.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [1., 1., 1.],\n",
            "          [1., 1., 1.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 1.],\n",
            "          [1., 1., 1.],\n",
            "          [0., 1., 1.]],\n",
            "\n",
            "         [[0., 1., 1.],\n",
            "          [0., 1., 1.],\n",
            "          [0., 1., 1.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 1., 1.],\n",
            "          [0., 1., 1.]]],\n",
            "\n",
            "\n",
            "        [[[1., 0., 0.],\n",
            "          [0., 1., 1.],\n",
            "          [1., 1., 1.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [1., 1., 1.],\n",
            "          [1., 1., 1.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 1., 1.],\n",
            "          [0., 0., 1.]]],\n",
            "\n",
            "\n",
            "        [[[1., 1., 1.],\n",
            "          [1., 1., 1.],\n",
            "          [1., 1., 1.]],\n",
            "\n",
            "         [[0., 1., 0.],\n",
            "          [0., 1., 1.],\n",
            "          [0., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.],\n",
            "          [0., 1., 1.],\n",
            "          [1., 1., 1.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.],\n",
            "          [1., 1., 1.],\n",
            "          [1., 1., 1.]],\n",
            "\n",
            "         [[1., 0., 0.],\n",
            "          [1., 1., 1.],\n",
            "          [1., 1., 1.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [1., 1., 1.],\n",
            "          [0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 1.],\n",
            "          [0., 1., 1.],\n",
            "          [0., 1., 1.]],\n",
            "\n",
            "         [[0., 1., 1.],\n",
            "          [0., 1., 1.],\n",
            "          [0., 1., 1.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 1.],\n",
            "          [0., 1., 1.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 1.],\n",
            "          [1., 1., 1.],\n",
            "          [0., 1., 1.]],\n",
            "\n",
            "         [[0., 1., 1.],\n",
            "          [1., 1., 1.],\n",
            "          [0., 1., 1.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [1., 1., 1.],\n",
            "          [0., 1., 1.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 1., 1.],\n",
            "          [1., 1., 1.],\n",
            "          [0., 0., 1.]],\n",
            "\n",
            "         [[1., 1., 1.],\n",
            "          [0., 1., 1.],\n",
            "          [1., 1., 1.]],\n",
            "\n",
            "         [[0., 1., 1.],\n",
            "          [1., 1., 1.],\n",
            "          [0., 1., 1.]]],\n",
            "\n",
            "\n",
            "        [[[1., 0., 1.],\n",
            "          [0., 1., 0.],\n",
            "          [1., 1., 1.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 1., 1.],\n",
            "          [1., 1., 1.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [1., 1., 1.]]],\n",
            "\n",
            "\n",
            "        [[[1., 1., 1.],\n",
            "          [1., 1., 1.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         [[1., 1., 1.],\n",
            "          [1., 1., 1.],\n",
            "          [1., 1., 0.]],\n",
            "\n",
            "         [[0., 1., 0.],\n",
            "          [1., 1., 0.],\n",
            "          [0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[1., 0., 0.],\n",
            "          [1., 1., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         [[1., 1., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         [[1., 1., 0.],\n",
            "          [1., 1., 0.],\n",
            "          [0., 1., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 1., 1.]],\n",
            "\n",
            "         [[0., 0., 1.],\n",
            "          [0., 1., 0.],\n",
            "          [1., 1., 1.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 1.]]],\n",
            "\n",
            "\n",
            "        [[[0., 1., 1.],\n",
            "          [1., 1., 1.],\n",
            "          [1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.],\n",
            "          [1., 1., 1.],\n",
            "          [1., 1., 0.]],\n",
            "\n",
            "         [[1., 1., 1.],\n",
            "          [1., 1., 1.],\n",
            "          [0., 1., 1.]]],\n",
            "\n",
            "\n",
            "        [[[0., 1., 0.],\n",
            "          [0., 1., 0.],\n",
            "          [1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.],\n",
            "          [1., 1., 1.],\n",
            "          [1., 0., 0.]],\n",
            "\n",
            "         [[1., 1., 1.],\n",
            "          [1., 1., 0.],\n",
            "          [0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[1., 1., 1.],\n",
            "          [1., 1., 0.],\n",
            "          [1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.],\n",
            "          [1., 1., 1.],\n",
            "          [1., 0., 1.]],\n",
            "\n",
            "         [[1., 0., 1.],\n",
            "          [1., 1., 0.],\n",
            "          [1., 1., 1.]]],\n",
            "\n",
            "\n",
            "        [[[1., 1., 1.],\n",
            "          [1., 1., 1.],\n",
            "          [1., 0., 1.]],\n",
            "\n",
            "         [[1., 1., 1.],\n",
            "          [1., 1., 1.],\n",
            "          [1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.],\n",
            "          [1., 1., 1.],\n",
            "          [1., 1., 1.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[1., 1., 0.],\n",
            "          [0., 1., 1.],\n",
            "          [0., 0., 1.]],\n",
            "\n",
            "         [[1., 0., 1.],\n",
            "          [0., 1., 1.],\n",
            "          [1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.],\n",
            "          [0., 1., 1.],\n",
            "          [1., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.],\n",
            "          [1., 1., 1.],\n",
            "          [1., 1., 1.]],\n",
            "\n",
            "         [[0., 1., 0.],\n",
            "          [1., 1., 1.],\n",
            "          [1., 1., 1.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [1., 1., 1.],\n",
            "          [0., 1., 1.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.],\n",
            "          [0., 1., 1.],\n",
            "          [0., 1., 1.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [1., 1., 1.],\n",
            "          [1., 1., 1.]],\n",
            "\n",
            "         [[0., 1., 1.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 1.]]],\n",
            "\n",
            "\n",
            "        [[[1., 1., 0.],\n",
            "          [1., 1., 1.],\n",
            "          [1., 1., 0.]],\n",
            "\n",
            "         [[1., 1., 1.],\n",
            "          [1., 1., 1.],\n",
            "          [1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 0.],\n",
            "          [1., 0., 1.],\n",
            "          [1., 1., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 1., 1.],\n",
            "          [1., 1., 1.],\n",
            "          [0., 1., 1.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 1., 0.],\n",
            "          [1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.],\n",
            "          [1., 1., 1.],\n",
            "          [0., 1., 1.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.],\n",
            "          [0., 0., 1.],\n",
            "          [1., 0., 1.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 1., 1.],\n",
            "          [1., 1., 1.]],\n",
            "\n",
            "         [[0., 0., 1.],\n",
            "          [0., 0., 0.],\n",
            "          [1., 0., 1.]]],\n",
            "\n",
            "\n",
            "        [[[1., 0., 1.],\n",
            "          [1., 1., 1.],\n",
            "          [1., 1., 1.]],\n",
            "\n",
            "         [[0., 1., 1.],\n",
            "          [1., 1., 1.],\n",
            "          [1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.],\n",
            "          [0., 1., 1.],\n",
            "          [1., 1., 1.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.],\n",
            "          [1., 0., 1.],\n",
            "          [0., 0., 1.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 1.],\n",
            "          [0., 1., 1.]],\n",
            "\n",
            "         [[0., 0., 1.],\n",
            "          [0., 0., 1.],\n",
            "          [0., 0., 1.]]]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply structured pruning\n",
        "✅ What is `ln_structured` Doing?\n",
        "\n",
        "- **n=2**: Uses L2 norm (Euclidean norm) for pruning. This means entire structures (e.g., channels or neurons) with the smallest L2 norm will be pruned.\n",
        "- **dim=0**: Prunes across the output dimension (e.g., removes entire filters in Conv2d or neurons in Linear)."
      ],
      "metadata": {
        "id": "QPG2uWlRoa6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.utils.prune as prune\n",
        "\n",
        "# Store pruning masks\n",
        "pruned_masks = {}\n",
        "\n",
        "# Apply and store pruning masks (structured pruning)\n",
        "def prune_model_structured(model, amount=0.2):\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
        "            # Avoid pruning critical layers\n",
        "            if 'features.0' in name:\n",
        "                continue\n",
        "\n",
        "            # Apply structured pruning (removes entire channels/neurons)\n",
        "            prune.ln_structured(module, name=\"weight\", amount=amount, n=2, dim=0)\n",
        "\n",
        "            # Store mask for later enforcement\n",
        "            pruned_masks[name] = (module.weight != 0).float()\n",
        "\n",
        "            # Make pruning permanent\n",
        "            prune.remove(module, \"weight\")\n",
        "\n",
        "            print(f\"Pruned {name}: Remaining shape {module.weight.shape}\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "pGymZlWOoeVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prune_model_structured(model, amount = 0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJW4xrsLpNJE",
        "outputId": "b9ee43a1-9280-407c-a8d8-5745017f0806"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pruned features.1.conv.0.0: Remaining shape torch.Size([32, 1, 3, 3])\n",
            "Pruned features.1.conv.1: Remaining shape torch.Size([16, 32, 1, 1])\n",
            "Pruned features.2.conv.0.0: Remaining shape torch.Size([96, 16, 1, 1])\n",
            "Pruned features.2.conv.1.0: Remaining shape torch.Size([96, 1, 3, 3])\n",
            "Pruned features.2.conv.2: Remaining shape torch.Size([24, 96, 1, 1])\n",
            "Pruned features.3.conv.0.0: Remaining shape torch.Size([144, 24, 1, 1])\n",
            "Pruned features.3.conv.1.0: Remaining shape torch.Size([144, 1, 3, 3])\n",
            "Pruned features.3.conv.2: Remaining shape torch.Size([24, 144, 1, 1])\n",
            "Pruned features.4.conv.0.0: Remaining shape torch.Size([144, 24, 1, 1])\n",
            "Pruned features.4.conv.1.0: Remaining shape torch.Size([144, 1, 3, 3])\n",
            "Pruned features.4.conv.2: Remaining shape torch.Size([32, 144, 1, 1])\n",
            "Pruned features.5.conv.0.0: Remaining shape torch.Size([192, 32, 1, 1])\n",
            "Pruned features.5.conv.1.0: Remaining shape torch.Size([192, 1, 3, 3])\n",
            "Pruned features.5.conv.2: Remaining shape torch.Size([32, 192, 1, 1])\n",
            "Pruned features.6.conv.0.0: Remaining shape torch.Size([192, 32, 1, 1])\n",
            "Pruned features.6.conv.1.0: Remaining shape torch.Size([192, 1, 3, 3])\n",
            "Pruned features.6.conv.2: Remaining shape torch.Size([32, 192, 1, 1])\n",
            "Pruned features.7.conv.0.0: Remaining shape torch.Size([192, 32, 1, 1])\n",
            "Pruned features.7.conv.1.0: Remaining shape torch.Size([192, 1, 3, 3])\n",
            "Pruned features.7.conv.2: Remaining shape torch.Size([64, 192, 1, 1])\n",
            "Pruned features.8.conv.0.0: Remaining shape torch.Size([384, 64, 1, 1])\n",
            "Pruned features.8.conv.1.0: Remaining shape torch.Size([384, 1, 3, 3])\n",
            "Pruned features.8.conv.2: Remaining shape torch.Size([64, 384, 1, 1])\n",
            "Pruned features.9.conv.0.0: Remaining shape torch.Size([384, 64, 1, 1])\n",
            "Pruned features.9.conv.1.0: Remaining shape torch.Size([384, 1, 3, 3])\n",
            "Pruned features.9.conv.2: Remaining shape torch.Size([64, 384, 1, 1])\n",
            "Pruned features.10.conv.0.0: Remaining shape torch.Size([384, 64, 1, 1])\n",
            "Pruned features.10.conv.1.0: Remaining shape torch.Size([384, 1, 3, 3])\n",
            "Pruned features.10.conv.2: Remaining shape torch.Size([64, 384, 1, 1])\n",
            "Pruned features.11.conv.0.0: Remaining shape torch.Size([384, 64, 1, 1])\n",
            "Pruned features.11.conv.1.0: Remaining shape torch.Size([384, 1, 3, 3])\n",
            "Pruned features.11.conv.2: Remaining shape torch.Size([96, 384, 1, 1])\n",
            "Pruned features.12.conv.0.0: Remaining shape torch.Size([576, 96, 1, 1])\n",
            "Pruned features.12.conv.1.0: Remaining shape torch.Size([576, 1, 3, 3])\n",
            "Pruned features.12.conv.2: Remaining shape torch.Size([96, 576, 1, 1])\n",
            "Pruned features.13.conv.0.0: Remaining shape torch.Size([576, 96, 1, 1])\n",
            "Pruned features.13.conv.1.0: Remaining shape torch.Size([576, 1, 3, 3])\n",
            "Pruned features.13.conv.2: Remaining shape torch.Size([96, 576, 1, 1])\n",
            "Pruned features.14.conv.0.0: Remaining shape torch.Size([576, 96, 1, 1])\n",
            "Pruned features.14.conv.1.0: Remaining shape torch.Size([576, 1, 3, 3])\n",
            "Pruned features.14.conv.2: Remaining shape torch.Size([160, 576, 1, 1])\n",
            "Pruned features.15.conv.0.0: Remaining shape torch.Size([960, 160, 1, 1])\n",
            "Pruned features.15.conv.1.0: Remaining shape torch.Size([960, 1, 3, 3])\n",
            "Pruned features.15.conv.2: Remaining shape torch.Size([160, 960, 1, 1])\n",
            "Pruned features.16.conv.0.0: Remaining shape torch.Size([960, 160, 1, 1])\n",
            "Pruned features.16.conv.1.0: Remaining shape torch.Size([960, 1, 3, 3])\n",
            "Pruned features.16.conv.2: Remaining shape torch.Size([160, 960, 1, 1])\n",
            "Pruned features.17.conv.0.0: Remaining shape torch.Size([960, 160, 1, 1])\n",
            "Pruned features.17.conv.1.0: Remaining shape torch.Size([960, 1, 3, 3])\n",
            "Pruned features.17.conv.2: Remaining shape torch.Size([320, 960, 1, 1])\n",
            "Pruned features.18.0: Remaining shape torch.Size([1280, 320, 1, 1])\n",
            "Pruned classifier.0: Remaining shape torch.Size([256, 1280])\n",
            "Pruned classifier.4: Remaining shape torch.Size([128, 256])\n",
            "Pruned classifier.8: Remaining shape torch.Size([64, 128])\n",
            "Pruned classifier.12: Remaining shape torch.Size([10, 64])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MobileNetV2(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2dNormActivation(\n",
              "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU6(inplace=True)\n",
              "    )\n",
              "    (1): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (2): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
              "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (3): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (4): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (5): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (6): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (7): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (8): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (9): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (10): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (11): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (12): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (13): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (14): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (15): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (16): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (17): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (18): Conv2dNormActivation(\n",
              "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU6(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=1280, out_features=256, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
              "    (5): ReLU()\n",
              "    (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (7): Dropout(p=0.5, inplace=False)\n",
              "    (8): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (9): ReLU()\n",
              "    (10): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (11): Dropout(p=0.5, inplace=False)\n",
              "    (12): Linear(in_features=64, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## See weights after pruning"
      ],
      "metadata": {
        "id": "zhlZcVV4vbIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the first convolutional layer\n",
        "conv1 = model.features[0]\n",
        "print(conv1)\n",
        "\n",
        "# Inspect its parameters (weights and bias)\n",
        "print(list(conv1.named_parameters()))"
      ],
      "metadata": {
        "id": "iK80_pIutDsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Linear) and name!=\"model.22.dfl.conv\" and name!=\"model.0.conv\":\n",
        "            print(module.weight)\n",
        "            print(pruned_masks[name])"
      ],
      "metadata": {
        "id": "wPws5M5wqPqB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05a17208-73eb-4cf9-8d1f-217bf96400e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.2708,  0.0258, -0.0234,  ...,  0.0166,  0.1309, -0.0230],\n",
            "        [-0.0000,  0.0000, -0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
            "        [ 0.1067, -0.0429, -0.0490,  ..., -0.0829,  0.3287, -0.0570],\n",
            "        ...,\n",
            "        [-0.0000,  0.0000, -0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
            "        [ 0.1093,  0.0201,  0.0281,  ...,  0.0527, -0.0101, -0.0232],\n",
            "        [ 0.1127, -0.2928,  0.0143,  ..., -0.0595,  0.1332, -0.0287]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')\n",
            "Parameter containing:\n",
            "tensor([[-0.0581,  0.2195,  0.0565,  ...,  0.1554, -0.0837,  0.0481],\n",
            "        [-0.0000,  0.0000, -0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0887,  0.0961,  0.0230,  ...,  0.0259,  0.1167,  0.0630],\n",
            "        ...,\n",
            "        [ 0.0231,  0.2158,  0.1313,  ...,  0.1434, -0.0173, -0.0501],\n",
            "        [ 0.0081,  0.1783, -0.1954,  ...,  0.0283,  0.1182, -0.0494],\n",
            "        [ 0.0066,  0.0786,  0.0238,  ...,  0.1585,  0.0414, -0.0113]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "        ...,\n",
            "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')\n",
            "Parameter containing:\n",
            "tensor([[ 0.0000,  0.0000, -0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
            "        [-0.1740,  0.0439, -0.1939,  ..., -0.0546, -0.0837, -0.1525],\n",
            "        [-0.0909, -0.0009, -0.0771,  ..., -0.2113,  0.0504, -0.1516],\n",
            "        ...,\n",
            "        [ 0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
            "        [-0.0975, -0.1090,  0.0512,  ..., -0.1599, -0.2273, -0.2043],\n",
            "        [ 0.1392, -0.0626,  0.2005,  ..., -0.0476,  0.0908, -0.0374]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')\n",
            "Parameter containing:\n",
            "tensor([[-0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000,\n",
            "         -0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,\n",
            "          0.0000, -0.0000, -0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,\n",
            "          0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,\n",
            "          0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
            "          0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000,\n",
            "         -0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
            "         -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000],\n",
            "        [ 0.2074,  0.1637,  0.2007, -0.1677, -0.2406, -0.0937, -0.0762,  0.0236,\n",
            "         -0.1992, -0.1104,  0.1966,  0.2357, -0.1043, -0.2119, -0.1384, -0.1701,\n",
            "         -0.2293, -0.2042,  0.2036,  0.1289, -0.0107,  0.2079,  0.1862, -0.1158,\n",
            "         -0.1866, -0.1105,  0.1985, -0.1737, -0.1311,  0.1455,  0.0942,  0.1698,\n",
            "          0.1878, -0.2066, -0.1298, -0.2011,  0.2113, -0.1393, -0.0729, -0.1962,\n",
            "          0.2283, -0.1380, -0.2129, -0.1359, -0.0584, -0.1080, -0.1267, -0.1283,\n",
            "         -0.1481,  0.1869, -0.1481, -0.1537, -0.0725,  0.2348,  0.1630,  0.1724,\n",
            "         -0.2518,  0.1301, -0.1148, -0.0264,  0.1986, -0.1916,  0.1794, -0.1957],\n",
            "        [-0.0000, -0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
            "         -0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000,\n",
            "          0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,\n",
            "          0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000,\n",
            "         -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         -0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000,\n",
            "         -0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
            "         -0.0000,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
            "         -0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
            "          0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
            "         -0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
            "          0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
            "         -0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000,\n",
            "         -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000],\n",
            "        [-0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,  0.0000,\n",
            "          0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000,  0.0000,\n",
            "          0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
            "         -0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
            "         -0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,\n",
            "          0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,\n",
            "          0.0000, -0.0000, -0.0000, -0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
            "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000,\n",
            "          0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         -0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
            "         -0.0000, -0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000,\n",
            "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,\n",
            "         -0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
            "         -0.0000, -0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
            "          0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.2235, -0.1933, -0.1437,  0.1485, -0.0018,  0.2062, -0.1277, -0.0688,\n",
            "          0.1715, -0.1674, -0.1628, -0.1193, -0.1384, -0.1806, -0.1484, -0.1639,\n",
            "          0.1438,  0.1649, -0.1759, -0.1500, -0.1291, -0.1046,  0.1837,  0.1375,\n",
            "         -0.1821, -0.0313, -0.1438, -0.2478,  0.2016, -0.1695, -0.1644, -0.1959,\n",
            "         -0.1865,  0.1838, -0.1490,  0.1620,  0.2413, -0.1046,  0.2174, -0.1717,\n",
            "         -0.1042, -0.1395,  0.1068, -0.0695, -0.1166,  0.1674,  0.1599,  0.1651,\n",
            "         -0.1469, -0.1717, -0.1445, -0.0734, -0.0775, -0.0228,  0.1452,  0.1959,\n",
            "         -0.1261,  0.1961, -0.1214, -0.0936,  0.1213,  0.1955, -0.1266, -0.1198],\n",
            "        [-0.1660, -0.1328, -0.1312, -0.1581,  0.1173, -0.1861, -0.1115,  0.1381,\n",
            "          0.2001, -0.1405, -0.1763,  0.1801, -0.1624,  0.1139, -0.0859,  0.0911,\n",
            "         -0.2395, -0.0980, -0.1467, -0.0827, -0.1134,  0.1385, -0.0997, -0.1162,\n",
            "          0.0983,  0.1532, -0.1421, -0.1548,  0.2092, -0.1555,  0.0892, -0.1419,\n",
            "         -0.1823, -0.1716,  0.1233,  0.1863, -0.1291,  0.1588, -0.1793,  0.1159,\n",
            "          0.0790, -0.1174, -0.1334,  0.0373,  0.2140, -0.1673, -0.0801, -0.0032,\n",
            "          0.1307,  0.1057,  0.1605,  0.2440,  0.1692,  0.1977, -0.1775, -0.1020,\n",
            "          0.1428,  0.1221,  0.1613, -0.1791, -0.1050, -0.0775, -0.1287, -0.2096],\n",
            "        [ 0.2210,  0.0695, -0.1576, -0.2440, -0.2550, -0.1248,  0.2388, -0.1022,\n",
            "         -0.2417, -0.1505, -0.1399, -0.1196,  0.1422, -0.2475, -0.1428,  0.1395,\n",
            "          0.0941,  0.1139, -0.1249, -0.1352,  0.2544, -0.0097, -0.1319, -0.1849,\n",
            "         -0.1765, -0.1681,  0.2113,  0.0638, -0.1200,  0.1699,  0.1702,  0.1000,\n",
            "          0.1705,  0.0835, -0.0364, -0.1775, -0.0699,  0.0277, -0.0063, -0.1123,\n",
            "          0.1471,  0.2098, -0.2770,  0.1698,  0.1801, -0.1734,  0.1346, -0.1479,\n",
            "          0.1692, -0.1076, -0.1400,  0.1697, -0.1611, -0.1267,  0.1643, -0.1496,\n",
            "         -0.1692, -0.2000, -0.2069,  0.2028, -0.1024, -0.1157, -0.1695, -0.1131],\n",
            "        [-0.0106, -0.1267,  0.1755, -0.0873, -0.2436,  0.1862,  0.1996,  0.1580,\n",
            "         -0.1843,  0.1184,  0.2044,  0.2439, -0.0634, -0.2014, -0.1843, -0.2396,\n",
            "         -0.1635, -0.1212,  0.1699,  0.0871,  0.2282, -0.0727, -0.0860,  0.1588,\n",
            "         -0.1735,  0.1209,  0.2343, -0.1853, -0.0975, -0.0737,  0.1661, -0.1156,\n",
            "          0.2058, -0.2097, -0.1868, -0.1716, -0.0421, -0.2008,  0.1807,  0.0512,\n",
            "         -0.0929, -0.1216, -0.2255, -0.0633, -0.1066, -0.2012, -0.1261,  0.1463,\n",
            "          0.1783,  0.1456, -0.2095, -0.1159, -0.1884,  0.0482, -0.0929, -0.1276,\n",
            "         -0.2645, -0.1221,  0.0977,  0.2130, -0.0802, -0.1318, -0.0884, -0.1972]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation (After pruning)"
      ],
      "metadata": {
        "id": "fskQ11bpt2RR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "avgLoss, acc, inf_time = evaluate_model(model, test_loader, criterion, device)\n",
        "# Print evaluation results\n",
        "print(f\"Average test Loss: {avgLoss:.4f}, Test Accuracy: {acc:.2f}%, Inference Time for {len(test_dataset)} images: {inf_time:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZfkkbRGt2AC",
        "outputId": "a7f8cd54-cce7-4121-aad6-efe85262d1ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average test Loss: 2.6684, Test Accuracy: 9.92%, Inference Time for 6000 images: 11.38 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tune the pruned model"
      ],
      "metadata": {
        "id": "0rp1b4eYuTvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Mask Enforcer\n",
        "mask_enforcer = MaskEnforcer(model, pruned_masks)\n",
        "\n",
        "# Set up optimizer and criterion\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4) # low learning rate to prevent zero weights from getting big values\n",
        "\n",
        "# Fine-tune the model while enforcing pruning mask\n",
        "train_model(model, train_loader, dev_loader, criterion, optimizer, num_epochs=5, mask_enforcer=mask_enforcer)"
      ],
      "metadata": {
        "id": "ahnVA7CouV_v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fb3883f-3e10-430c-e535-093c33f5c5a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 782/782 [02:14<00:00,  5.81it/s, train_acc=27.2, train_loss=0.0329]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 2.1012, Train Acc: 27.18%, Dev Loss: 1.6826, Dev Acc: 38.12%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 782/782 [02:13<00:00,  5.86it/s, train_acc=36.6, train_loss=0.0277]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train Loss: 1.7738, Train Acc: 36.61%, Dev Loss: 1.5351, Dev Acc: 42.98%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 782/782 [02:14<00:00,  5.82it/s, train_acc=40.6, train_loss=0.0258]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Train Loss: 1.6518, Train Acc: 40.60%, Dev Loss: 1.4454, Dev Acc: 45.60%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 782/782 [02:15<00:00,  5.76it/s, train_acc=43.2, train_loss=0.0245]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Train Loss: 1.5666, Train Acc: 43.21%, Dev Loss: 1.3865, Dev Acc: 46.85%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 782/782 [02:20<00:00,  5.57it/s, train_acc=44.7, train_loss=0.0236]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train Loss: 1.5067, Train Acc: 44.67%, Dev Loss: 1.3449, Dev Acc: 47.77%\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## fine tune for more epochs"
      ],
      "metadata": {
        "id": "pwV1JLPnR52B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tune the model while enforcing pruning mask\n",
        "train_model(model, train_loader, dev_loader, criterion, optimizer, num_epochs=5, mask_enforcer=mask_enforcer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYIGtLbJR-UQ",
        "outputId": "74c75dc4-2918-4392-b64b-23d8b1259f95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 782/782 [02:16<00:00,  5.73it/s, train_acc=46.1, train_loss=0.0228]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 1.4581, Train Acc: 46.08%, Dev Loss: 1.3118, Dev Acc: 48.62%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 782/782 [02:15<00:00,  5.77it/s, train_acc=47, train_loss=0.0222]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train Loss: 1.4218, Train Acc: 46.99%, Dev Loss: 1.2870, Dev Acc: 49.35%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 782/782 [02:14<00:00,  5.80it/s, train_acc=47.6, train_loss=0.0218]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Train Loss: 1.3918, Train Acc: 47.62%, Dev Loss: 1.2584, Dev Acc: 50.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 782/782 [02:13<00:00,  5.84it/s, train_acc=48.2, train_loss=0.0213]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Train Loss: 1.3618, Train Acc: 48.20%, Dev Loss: 1.2409, Dev Acc: 50.33%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 782/782 [02:15<00:00,  5.78it/s, train_acc=48.5, train_loss=0.021]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train Loss: 1.3421, Train Acc: 48.45%, Dev Loss: 1.2263, Dev Acc: 50.73%\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check if pruned weights still zero after fine-tuning"
      ],
      "metadata": {
        "id": "35-fHqu3quHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Linear) and name!=\"model.22.dfl.conv\" and name!=\"model.0.conv\":\n",
        "            print(module.weight)\n",
        "            print(pruned_masks[name])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82huh1sRqz3k",
        "outputId": "924e0d23-470d-47df-f312-faf470e7b89a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.2708,  0.0093,  0.0333,  ...,  0.0166,  0.1392, -0.0230],\n",
            "        [-0.0000,  0.0000, -0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
            "        [ 0.1067, -0.0475, -0.0326,  ..., -0.0829,  0.3345, -0.0570],\n",
            "        ...,\n",
            "        [-0.0000,  0.0000, -0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
            "        [ 0.1093, -0.0117, -0.0108,  ...,  0.0527, -0.0345, -0.0232],\n",
            "        [ 0.1127, -0.2593,  0.0379,  ..., -0.0595,  0.1224, -0.0287]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')\n",
            "Parameter containing:\n",
            "tensor([[-0.0421,  0.2191,  0.0641,  ...,  0.1631, -0.0823,  0.0510],\n",
            "        [ 0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
            "        [ 0.0868,  0.0919,  0.0260,  ...,  0.0540,  0.1047,  0.0538],\n",
            "        ...,\n",
            "        [ 0.0239,  0.1905,  0.1182,  ...,  0.1191, -0.0305, -0.0483],\n",
            "        [-0.0168,  0.1706, -0.1975,  ...,  0.0072,  0.1069, -0.0422],\n",
            "        [-0.0119,  0.0591,  0.0240,  ...,  0.1467,  0.0399, -0.0022]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "        ...,\n",
            "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')\n",
            "Parameter containing:\n",
            "tensor([[ 0.0000,  0.0000, -0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
            "        [-0.1573,  0.0061, -0.1406,  ..., -0.0643, -0.0750, -0.1510],\n",
            "        [-0.1520, -0.0142, -0.0518,  ..., -0.2002,  0.0468, -0.1266],\n",
            "        ...,\n",
            "        [ 0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
            "        [-0.1089, -0.1235,  0.0887,  ..., -0.1814, -0.2570, -0.1654],\n",
            "        [ 0.1150, -0.0863,  0.2197,  ..., -0.0672,  0.0328, -0.0264]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')\n",
            "Parameter containing:\n",
            "tensor([[-0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
            "         -0.0000,  0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
            "          0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
            "          0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         -0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000,  0.0000,\n",
            "          0.0000, -0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
            "         -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000,  0.0000,\n",
            "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.2289,  0.1404,  0.2309, -0.2192, -0.2311, -0.0321, -0.0412, -0.0065,\n",
            "         -0.1851, -0.1941,  0.1426,  0.2522, -0.1718, -0.2442, -0.2407, -0.2399,\n",
            "         -0.2289, -0.1665,  0.1512,  0.1612,  0.0305,  0.2331,  0.1937, -0.1125,\n",
            "         -0.2559, -0.1417,  0.2650, -0.2761, -0.1601,  0.1374,  0.0628,  0.1159,\n",
            "          0.2169, -0.2152, -0.2114, -0.1720,  0.2173, -0.1785, -0.0252, -0.2125,\n",
            "          0.1949, -0.1168, -0.2328, -0.1771, -0.1454, -0.1460, -0.1638, -0.0863,\n",
            "         -0.0866,  0.2193, -0.1814, -0.1023, -0.0581,  0.2673,  0.1231,  0.1041,\n",
            "         -0.2859,  0.1603, -0.1538,  0.0241,  0.1592, -0.2282,  0.1636, -0.2647],\n",
            "        [ 0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
            "          0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
            "         -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         -0.0000, -0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000,\n",
            "         -0.0000, -0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
            "          0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000,\n",
            "         -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000, -0.0000,\n",
            "          0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000,\n",
            "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,\n",
            "          0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
            "         -0.0000, -0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,  0.0000,\n",
            "         -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000],\n",
            "        [ 0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,\n",
            "          0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
            "         -0.0000,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
            "         -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
            "         -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
            "          0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
            "          0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000,\n",
            "         -0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000],\n",
            "        [-0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000,\n",
            "          0.0000, -0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000,\n",
            "          0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000,\n",
            "         -0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
            "         -0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
            "          0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000,\n",
            "         -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
            "        [ 0.2844, -0.1477, -0.1043,  0.0660,  0.0230,  0.1908, -0.0336, -0.1450,\n",
            "          0.1316, -0.2030, -0.2375, -0.0190, -0.1299, -0.1949, -0.1754, -0.1876,\n",
            "          0.0479,  0.2525, -0.2705, -0.1487, -0.0264, -0.0143,  0.2184,  0.1109,\n",
            "         -0.2047, -0.1085, -0.0486, -0.1934,  0.1235, -0.1094, -0.2130, -0.2686,\n",
            "         -0.0998,  0.0841, -0.1932,  0.1351,  0.2931, -0.1267,  0.2697, -0.2289,\n",
            "         -0.0596, -0.0642,  0.0563, -0.1363, -0.1141,  0.0577,  0.1304,  0.1447,\n",
            "         -0.0352, -0.0853, -0.2292,  0.0158, -0.1247, -0.0253,  0.0616,  0.1133,\n",
            "         -0.2102,  0.1738, -0.1919, -0.0476,  0.0148,  0.0959, -0.1057, -0.1451],\n",
            "        [-0.0686, -0.1364,  0.0042, -0.2227,  0.0459, -0.1145, -0.0264,  0.0498,\n",
            "          0.1319, -0.1501, -0.2446,  0.2200, -0.1795,  0.0570, -0.1681,  0.0708,\n",
            "         -0.2729, -0.0107, -0.2257,  0.0108, -0.0332,  0.2187, -0.0187, -0.1374,\n",
            "          0.0244,  0.0615, -0.0370, -0.1824,  0.1371, -0.1428,  0.0075, -0.2090,\n",
            "         -0.0961, -0.2285,  0.0754,  0.1350, -0.0363,  0.1135, -0.0806,  0.0905,\n",
            "          0.1182, -0.0356, -0.1927, -0.0489,  0.1838, -0.2351, -0.1498,  0.0564,\n",
            "          0.2289,  0.1753,  0.0621,  0.2885,  0.0912,  0.2288, -0.2459, -0.1795,\n",
            "          0.0486,  0.1061,  0.1188, -0.0735, -0.1844, -0.1560, -0.1323, -0.2252],\n",
            "        [ 0.2893,  0.0724, -0.1019, -0.2583, -0.2358, -0.0885,  0.3129, -0.1695,\n",
            "         -0.1859, -0.1804, -0.2162, -0.0192,  0.1051, -0.2511, -0.1904,  0.1188,\n",
            "         -0.0029,  0.2105, -0.2000, -0.1466,  0.2989,  0.0675, -0.0401, -0.1432,\n",
            "         -0.2181, -0.2317,  0.2467,  0.0553, -0.1850,  0.1272,  0.0964,  0.0074,\n",
            "          0.2451, -0.0116, -0.0828, -0.1583,  0.0086,  0.0727,  0.0708, -0.1395,\n",
            "          0.1480,  0.2707, -0.2520,  0.0860,  0.1730, -0.2280,  0.1481, -0.0983,\n",
            "          0.2574, -0.0223, -0.2259,  0.2206, -0.2152, -0.0863,  0.0809, -0.2298,\n",
            "         -0.2460, -0.2065, -0.2030,  0.2640, -0.1894, -0.2015, -0.2044, -0.1447],\n",
            "        [ 0.0323, -0.1302,  0.2186, -0.2361, -0.2615,  0.1734,  0.2361,  0.1057,\n",
            "         -0.1816,  0.0784,  0.1566,  0.2480, -0.1112, -0.2000, -0.2219, -0.1902,\n",
            "         -0.1823, -0.0736,  0.1194,  0.1377,  0.2338, -0.0349, -0.0392,  0.0980,\n",
            "         -0.2066,  0.0836,  0.2561, -0.2243, -0.1196, -0.0817,  0.1224, -0.1316,\n",
            "          0.2301, -0.2204, -0.1673, -0.1814,  0.0036, -0.1856,  0.1975,  0.0703,\n",
            "         -0.0597, -0.0880, -0.2640, -0.0949, -0.0222, -0.2138, -0.1780,  0.1308,\n",
            "          0.1971,  0.1674, -0.2396, -0.0669, -0.1760,  0.1765, -0.1154, -0.1522,\n",
            "         -0.2864, -0.1262,  0.0992,  0.2335, -0.1128, -0.1585, -0.0836, -0.2418]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify pruning levels\n",
        "def check_sparsity(model):\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
        "            total = module.weight.numel()\n",
        "            nonzero = module.weight.nonzero().size(0)\n",
        "            sparsity = 100 * (1 - nonzero / total)\n",
        "            print(f\"{name}: {sparsity:.2f}% sparsity\")\n",
        "\n",
        "check_sparsity(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G864csSiubBA",
        "outputId": "43bb8399-ff8b-48b3-eb5e-729229fcb216"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "features.0.0: 0.00% sparsity\n",
            "features.1.conv.0.0: 50.00% sparsity\n",
            "features.1.conv.1: 50.00% sparsity\n",
            "features.2.conv.0.0: 50.00% sparsity\n",
            "features.2.conv.1.0: 50.00% sparsity\n",
            "features.2.conv.2: 50.00% sparsity\n",
            "features.3.conv.0.0: 50.00% sparsity\n",
            "features.3.conv.1.0: 50.00% sparsity\n",
            "features.3.conv.2: 50.00% sparsity\n",
            "features.4.conv.0.0: 50.00% sparsity\n",
            "features.4.conv.1.0: 50.00% sparsity\n",
            "features.4.conv.2: 50.00% sparsity\n",
            "features.5.conv.0.0: 50.00% sparsity\n",
            "features.5.conv.1.0: 50.00% sparsity\n",
            "features.5.conv.2: 50.00% sparsity\n",
            "features.6.conv.0.0: 50.00% sparsity\n",
            "features.6.conv.1.0: 50.00% sparsity\n",
            "features.6.conv.2: 50.00% sparsity\n",
            "features.7.conv.0.0: 50.00% sparsity\n",
            "features.7.conv.1.0: 50.00% sparsity\n",
            "features.7.conv.2: 50.00% sparsity\n",
            "features.8.conv.0.0: 50.00% sparsity\n",
            "features.8.conv.1.0: 50.00% sparsity\n",
            "features.8.conv.2: 50.00% sparsity\n",
            "features.9.conv.0.0: 50.00% sparsity\n",
            "features.9.conv.1.0: 50.00% sparsity\n",
            "features.9.conv.2: 50.00% sparsity\n",
            "features.10.conv.0.0: 50.00% sparsity\n",
            "features.10.conv.1.0: 50.00% sparsity\n",
            "features.10.conv.2: 50.00% sparsity\n",
            "features.11.conv.0.0: 50.00% sparsity\n",
            "features.11.conv.1.0: 50.00% sparsity\n",
            "features.11.conv.2: 50.00% sparsity\n",
            "features.12.conv.0.0: 50.00% sparsity\n",
            "features.12.conv.1.0: 50.00% sparsity\n",
            "features.12.conv.2: 50.00% sparsity\n",
            "features.13.conv.0.0: 50.00% sparsity\n",
            "features.13.conv.1.0: 50.00% sparsity\n",
            "features.13.conv.2: 50.00% sparsity\n",
            "features.14.conv.0.0: 50.00% sparsity\n",
            "features.14.conv.1.0: 50.00% sparsity\n",
            "features.14.conv.2: 50.00% sparsity\n",
            "features.15.conv.0.0: 50.00% sparsity\n",
            "features.15.conv.1.0: 50.00% sparsity\n",
            "features.15.conv.2: 50.00% sparsity\n",
            "features.16.conv.0.0: 50.00% sparsity\n",
            "features.16.conv.1.0: 50.00% sparsity\n",
            "features.16.conv.2: 50.00% sparsity\n",
            "features.17.conv.0.0: 50.00% sparsity\n",
            "features.17.conv.1.0: 50.00% sparsity\n",
            "features.17.conv.2: 50.00% sparsity\n",
            "features.18.0: 50.00% sparsity\n",
            "classifier.0: 50.00% sparsity\n",
            "classifier.4: 50.00% sparsity\n",
            "classifier.8: 50.00% sparsity\n",
            "classifier.12: 50.00% sparsity\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate model after fine tuning"
      ],
      "metadata": {
        "id": "2DwxZ0ZnziNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "avgLoss, acc, inf_time = evaluate_model(model, test_loader, criterion, device)\n",
        "# Print evaluation results\n",
        "print(f\"Average test Loss: {avgLoss:.4f}, Test Accuracy: {acc:.2f}%, Inference Time for {len(test_dataset)} images: {inf_time:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b--_-pzjziAh",
        "outputId": "917b63ba-6d8e-43c7-b250-1ace65b9c400"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average test Loss: 1.2260, Test Accuracy: 51.05%, Inference Time for 6000 images: 13.09 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert pruned layers to sparse"
      ],
      "metadata": {
        "id": "Cgq7G8DmVddV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, module in model.named_modules():\n",
        "    if isinstance(module, torch.nn.Linear) or isinstance(module, torch.nn.Conv2d):\n",
        "        module.weight = torch.nn.Parameter(module.weight.to_sparse())  # Convert to sparse format"
      ],
      "metadata": {
        "id": "gBARWnkeVjQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Linear) and name!=\"model.22.dfl.conv\" and name!=\"model.0.conv\":\n",
        "            print(module.weight)\n",
        "            print(pruned_masks[name])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GZxR3ivVniy",
        "outputId": "95cd48d7-47c0-4236-f4e2-9f1721e07b39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor(indices=tensor([[   0,    0,    0,  ...,  255,  255,  255],\n",
            "                       [   0,    1,    2,  ..., 1277, 1278, 1279]]),\n",
            "       values=tensor([ 0.2708,  0.0093,  0.0333,  ..., -0.0595,  0.1224,\n",
            "                      -0.0287]),\n",
            "       device='cuda:0', size=(256, 1280), nnz=163840, layout=torch.sparse_coo,\n",
            "       requires_grad=True)\n",
            "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')\n",
            "Parameter containing:\n",
            "tensor(indices=tensor([[  0,   0,   0,  ..., 127, 127, 127],\n",
            "                       [  0,   1,   2,  ..., 253, 254, 255]]),\n",
            "       values=tensor([-0.0421,  0.2191,  0.0641,  ...,  0.1467,  0.0399,\n",
            "                      -0.0022]),\n",
            "       device='cuda:0', size=(128, 256), nnz=16384, layout=torch.sparse_coo,\n",
            "       requires_grad=True)\n",
            "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "        ...,\n",
            "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')\n",
            "Parameter containing:\n",
            "tensor(indices=tensor([[  1,   1,   1,  ...,  63,  63,  63],\n",
            "                       [  0,   1,   2,  ..., 125, 126, 127]]),\n",
            "       values=tensor([-0.1573,  0.0061, -0.1406,  ..., -0.0672,  0.0328,\n",
            "                      -0.0264]),\n",
            "       device='cuda:0', size=(64, 128), nnz=4096, layout=torch.sparse_coo,\n",
            "       requires_grad=True)\n",
            "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')\n",
            "Parameter containing:\n",
            "tensor(indices=tensor([[ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
            "                         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
            "                         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
            "                         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
            "                         1,  1,  1,  1,  1,  1,  1,  1,  6,  6,  6,  6,  6,  6,\n",
            "                         6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
            "                         6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
            "                         6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
            "                         6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
            "                         6,  6,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
            "                         7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
            "                         7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
            "                         7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
            "                         7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,\n",
            "                         8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
            "                         8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
            "                         8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
            "                         8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
            "                         8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
            "                         9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
            "                         9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
            "                         9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
            "                         9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9],\n",
            "                       [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,\n",
            "                        14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27,\n",
            "                        28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41,\n",
            "                        42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55,\n",
            "                        56, 57, 58, 59, 60, 61, 62, 63,  0,  1,  2,  3,  4,  5,\n",
            "                         6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
            "                        20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
            "                        34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47,\n",
            "                        48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61,\n",
            "                        62, 63,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11,\n",
            "                        12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25,\n",
            "                        26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39,\n",
            "                        40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
            "                        54, 55, 56, 57, 58, 59, 60, 61, 62, 63,  0,  1,  2,  3,\n",
            "                         4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
            "                        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31,\n",
            "                        32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45,\n",
            "                        46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59,\n",
            "                        60, 61, 62, 63,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9,\n",
            "                        10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23,\n",
            "                        24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37,\n",
            "                        38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
            "                        52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]]),\n",
            "       values=tensor([ 0.2289,  0.1404,  0.2309, -0.2192, -0.2311, -0.0321,\n",
            "                      -0.0412, -0.0065, -0.1851, -0.1941,  0.1426,  0.2522,\n",
            "                      -0.1718, -0.2442, -0.2407, -0.2399, -0.2289, -0.1665,\n",
            "                       0.1512,  0.1612,  0.0305,  0.2331,  0.1937, -0.1125,\n",
            "                      -0.2559, -0.1417,  0.2650, -0.2761, -0.1601,  0.1374,\n",
            "                       0.0628,  0.1159,  0.2169, -0.2152, -0.2114, -0.1720,\n",
            "                       0.2173, -0.1785, -0.0252, -0.2125,  0.1949, -0.1168,\n",
            "                      -0.2328, -0.1771, -0.1454, -0.1460, -0.1638, -0.0863,\n",
            "                      -0.0866,  0.2193, -0.1814, -0.1023, -0.0581,  0.2673,\n",
            "                       0.1231,  0.1041, -0.2859,  0.1603, -0.1538,  0.0241,\n",
            "                       0.1592, -0.2282,  0.1636, -0.2647,  0.2844, -0.1477,\n",
            "                      -0.1043,  0.0660,  0.0230,  0.1908, -0.0336, -0.1450,\n",
            "                       0.1316, -0.2030, -0.2375, -0.0190, -0.1299, -0.1949,\n",
            "                      -0.1754, -0.1876,  0.0479,  0.2525, -0.2705, -0.1487,\n",
            "                      -0.0264, -0.0143,  0.2184,  0.1109, -0.2047, -0.1085,\n",
            "                      -0.0486, -0.1934,  0.1235, -0.1094, -0.2130, -0.2686,\n",
            "                      -0.0998,  0.0841, -0.1932,  0.1351,  0.2931, -0.1267,\n",
            "                       0.2697, -0.2289, -0.0596, -0.0642,  0.0563, -0.1363,\n",
            "                      -0.1141,  0.0577,  0.1304,  0.1447, -0.0352, -0.0853,\n",
            "                      -0.2292,  0.0158, -0.1247, -0.0253,  0.0616,  0.1133,\n",
            "                      -0.2102,  0.1738, -0.1919, -0.0476,  0.0148,  0.0959,\n",
            "                      -0.1057, -0.1451, -0.0686, -0.1364,  0.0042, -0.2227,\n",
            "                       0.0459, -0.1145, -0.0264,  0.0498,  0.1319, -0.1501,\n",
            "                      -0.2446,  0.2200, -0.1795,  0.0570, -0.1681,  0.0708,\n",
            "                      -0.2729, -0.0107, -0.2257,  0.0108, -0.0332,  0.2187,\n",
            "                      -0.0187, -0.1374,  0.0244,  0.0615, -0.0370, -0.1824,\n",
            "                       0.1371, -0.1428,  0.0075, -0.2090, -0.0961, -0.2285,\n",
            "                       0.0754,  0.1350, -0.0363,  0.1135, -0.0806,  0.0905,\n",
            "                       0.1182, -0.0356, -0.1927, -0.0489,  0.1838, -0.2351,\n",
            "                      -0.1498,  0.0564,  0.2289,  0.1753,  0.0621,  0.2885,\n",
            "                       0.0912,  0.2288, -0.2459, -0.1795,  0.0486,  0.1061,\n",
            "                       0.1188, -0.0735, -0.1844, -0.1560, -0.1323, -0.2252,\n",
            "                       0.2893,  0.0724, -0.1019, -0.2583, -0.2358, -0.0885,\n",
            "                       0.3129, -0.1695, -0.1859, -0.1804, -0.2162, -0.0192,\n",
            "                       0.1051, -0.2511, -0.1904,  0.1188, -0.0029,  0.2105,\n",
            "                      -0.2000, -0.1466,  0.2989,  0.0675, -0.0401, -0.1432,\n",
            "                      -0.2181, -0.2317,  0.2467,  0.0553, -0.1850,  0.1272,\n",
            "                       0.0964,  0.0074,  0.2451, -0.0116, -0.0828, -0.1583,\n",
            "                       0.0086,  0.0727,  0.0708, -0.1395,  0.1480,  0.2707,\n",
            "                      -0.2520,  0.0860,  0.1730, -0.2280,  0.1481, -0.0983,\n",
            "                       0.2574, -0.0223, -0.2259,  0.2206, -0.2152, -0.0863,\n",
            "                       0.0809, -0.2298, -0.2460, -0.2065, -0.2030,  0.2640,\n",
            "                      -0.1894, -0.2015, -0.2044, -0.1447,  0.0323, -0.1302,\n",
            "                       0.2186, -0.2361, -0.2615,  0.1734,  0.2361,  0.1057,\n",
            "                      -0.1816,  0.0784,  0.1566,  0.2480, -0.1112, -0.2000,\n",
            "                      -0.2219, -0.1902, -0.1823, -0.0736,  0.1194,  0.1377,\n",
            "                       0.2338, -0.0349, -0.0392,  0.0980, -0.2066,  0.0836,\n",
            "                       0.2561, -0.2243, -0.1196, -0.0817,  0.1224, -0.1316,\n",
            "                       0.2301, -0.2204, -0.1673, -0.1814,  0.0036, -0.1856,\n",
            "                       0.1975,  0.0703, -0.0597, -0.0880, -0.2640, -0.0949,\n",
            "                      -0.0222, -0.2138, -0.1780,  0.1308,  0.1971,  0.1674,\n",
            "                      -0.2396, -0.0669, -0.1760,  0.1765, -0.1154, -0.1522,\n",
            "                      -0.2864, -0.1262,  0.0992,  0.2335, -0.1128, -0.1585,\n",
            "                      -0.0836, -0.2418]),\n",
            "       device='cuda:0', size=(10, 64), nnz=320, layout=torch.sparse_coo,\n",
            "       requires_grad=True)\n",
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path = '/content/drive/My Drive/saved_models/pytorch_models/mobilenet_sparse.pth'\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "print(f\"Model weights saved successfully at: {model_save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCvTxtEoWAdS",
        "outputId": "330e4838-631a-4f55-97a9-e17a962e1279"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model weights saved successfully at: /content/drive/My Drive/saved_models/pytorch_models/mobilenet_sparse.pth\n"
          ]
        }
      ]
    }
  ]
}